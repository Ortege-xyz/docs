[
  {
    "objectID": "studio/charts.html",
    "href": "studio/charts.html",
    "title": "Charts",
    "section": "",
    "text": "Charts\nThe Charts feature in Ortege Studio is a powerful tool for creating and managing a wide range of data visualizations. From simple line charts to complex geospatial visualizations, Charts enable users to turn data into insightful and interactive visual representations.\n\nAccessing Charts\nScreenshot needed: Accessing Charts from the Studio menu\n\nNavigation: Charts can be accessed from the main Ortege Studio menu, typically under the “Charts” section.\nCharts Page: This page lists all the created charts, showing details like the chart name, type, and the dataset it’s based on.\n\n\n\nCreating a New Chart\nScreenshot needed: Creating a new chart interface\n\nChart Creation: To create a new chart, click on the “+” or “New” button.\nChoosing a Dataset: First, select the dataset you want to visualize.\nSelecting Chart Type: Ortege Studio offers a variety of chart types. Choose one that best suits the data and the insights you wish to convey.\n\n\n\nChart Customization\nScreenshot needed: Chart customization interface\n\nCustomization Panel: Once you’ve selected the chart type, you’ll be taken to the customization panel. Here, you can define and tweak various aspects of your chart.\nSetting Metrics and Dimensions: Choose the metrics and dimensions from your dataset that you want to visualize.\nChart Options: Adjust settings such as filters, sorting, and chart-specific options to fine-tune your visualization.\n\n\n\nVisualizing the Data\nScreenshot needed: Finished chart visualization\n\nPreviewing the Chart: As you customize your chart, you can preview it in real-time. This feature helps in iteratively building your visualization.\nInteractivity: Many chart types in Ortege Studio are interactive, allowing users to hover over elements to see more details or click to drill down further.\n\n\n\nSaving and Sharing Charts\nScreenshot needed: Saving and sharing options for a chart\n\nSaving the Chart: Once satisfied with the visualization, you can save the chart, giving it a descriptive name and, if necessary, adding it to a dashboard.\nSharing: Ortege Studio allows you to share charts with others, either through direct links or by adding them to shared dashboards.\n\n\n\nAdvanced Features\nScreenshot needed: Advanced features in chart creation, like SQL editing\n\nSQL Editing: For advanced users, Ortege Studio allows editing the underlying SQL of a chart, providing more control over the data and its presentation.\nCustom Visualization Plugins: Users can extend Ortege Studio’s visualization capabilities by adding custom chart plugins.\n\n\n\nConclusion\nThe Charts feature in Ortege Studio is a cornerstone of its data visualization capabilities. It offers a broad spectrum of options to represent data graphically, catering to both novice users and data experts. With its intuitive interface and wide range of customization options, Charts empower users to transform data into actionable insights."
  },
  {
    "objectID": "studio/dashboards.html",
    "href": "studio/dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "Dashboards\nDashboards in Ortege Studio are powerful tools for combining various charts and visualizations into a single, interactive, and dynamic view. They provide a holistic view of your data, allowing for efficient monitoring and insightful decision-making.\n\nAccessing Dashboards\nScreenshot needed: Accessing Dashboards from the Ortege Studio menu\n\nNavigation: You can find Dashboards in the main Ortege Studio menu, often under a section labeled “Dashboards.”\nDashboard Page: This page displays all the available dashboards, with details like the dashboard name, owner, and last modified date.\n\n\n\nCreating a New Dashboard\nScreenshot needed: Creating a new dashboard interface\n\nStarting a New Dashboard: To create a new dashboard, click on the “+” or “New” button.\nNaming the Dashboard: Assign a name and optionally a description to your new dashboard for easy identification.\n\n\n\nAdding Charts to Dashboard\nScreenshot needed: Adding charts to a dashboard\n\nChart Selection: In the dashboard edit mode, you can add charts that you’ve previously created in the Charts section.\nPositioning and Resizing: Drag and drop the charts into your desired position on the dashboard. You can also resize them for optimal layout and visibility.\n\n\n\nCustomizing the Dashboard\nScreenshot needed: Customizing the dashboard layout and settings\n\nLayout Customization: You can customize the layout of the dashboard, organizing charts and components in a way that best tells your data story.\nFilter Options: Add interactive filters to the dashboard that allow users to dynamically change the data displayed across multiple charts.\n\n\n\nInteracting with Dashboards\nScreenshot needed: Interacting with a live dashboard\n\nData Interaction: Users can interact with the data in various ways, like hovering over charts to see detailed information or clicking on elements to drill down.\nFiltering: Apply filters to refine the data displayed on the entire dashboard or specific charts.\n\n\n\nSharing and Exporting\nScreenshot needed: Sharing and exporting options for a dashboard\n\nSharing Dashboards: Dashboards can be shared with others through direct links or by providing access within Superset.\nExporting: Dashboards can also be exported for external use, either as images or in other formats, depending on Ortege Studio’s configuration.\n\n\n\nSecurity and Permissions\nScreenshot needed: Dashboard’s security settings\n\nAccess Control: You can control who has access to view or edit the dashboard, ensuring data security and integrity.\nRow-Level Security: If set up, row-level security will apply to all the charts in the dashboard, ensuring users only see the data they are authorized to view.\n\n\n\nConclusion\nDashboards in Ortege Studio are an essential feature for synthesizing complex data into comprehensible and actionable insights. They offer a platform to narrate a data story, combining different visual elements into a cohesive and interactive experience."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Ortege",
    "section": "",
    "text": "Ortege Studio\nOrtege Studio: Ortege Studio is a cutting-edge analytics platform designed to empower users with unparalleled insights into their data through an extensive array of visualization options, with over 30 types of charts available to cater to a wide range of analytical needs. At its core, Ortege Studio harnesses the power of predictive analytics, enabling businesses to forecast trends and make data-driven decisions with greater accuracy. The platform stands out by offering the flexibility to connect to a multitude of data sources, allowing users to seamlessly integrate and harmonize disparate data sets. Thanks to its sophisticated Data Manipulation Language (DML) capabilities, users can effortlessly join, query, and analyze their data across various sources, all within a user-friendly interface. Ortege Studio is the ultimate tool for organizations looking to unlock the full potential of their data, offering a comprehensive solution that blends advanced analytics with intuitive data integration and exploration features.\n\n\nOrtege Lakehouse\nOrtege Lakehouse: Ortege Lakehouse is an innovative data management platform engineered for the era of big data, embodying cutting-edge technology to meet the demands of extensive datasets and complex analytics. Designed to effortlessly integrate with MLOps frameworks and Large Language Models (LLMs), it provides a robust foundation for advanced machine learning projects and predictive analytics. A standout feature of Ortege Lakehouse is its comprehensive datasets for a wide array of the market’s most popular blockchains. With these datasets, users gain immediate access to a wealth of blockchain data, streamlined for analysis. Moreover, our platform introduces “silver and gold layers” – a feature that simplifies the querying of complex blockchain datasets, ensuring that even the most intricate data becomes easily navigable. This makes Ortege Lakehouse an indispensable tool for organizations looking to leverage blockchain data for insightful analysis, predictive modeling, and strategic decision-making, all while ensuring seamless workflow integration and operational efficiency.\n\n\nOrtegeGPT\nOrtegeGPT: OrtegeGPT, our latest innovation currently under rigorous development, is poised to redefine the way we interact with blockchain data. This cutting-edge Large Language Model (LLM) is being meticulously crafted to provide real-time, intuitive querying capabilities across a vast spectrum of supported blockchains. Upon completion, OrtegeGPT will empower users to effortlessly extract insights, perform complex analyses, and obtain answers to virtually any query about the blockchain ecosystems we support. Recognizing the diverse needs of our users, OrtegeGPT is also designed with flexibility in mind, allowing organizations to integrate their proprietary data seamlessly, thereby enriching their analytical capabilities and insights. Furthermore, OrtegeGPT is set to offer multiple distribution channels, ensuring that our advanced LLM can be accessed conveniently, whether through direct API integrations, web interfaces, or customized applications. This ambitious feature signifies our commitment to pioneering in the blockchain analytics space, offering an unparalleled tool that combines the latest in AI technology with the expansive world of blockchain data.\n\n\nQuick Start\nTo familiarize yourself with our documentation approach, please read the Quick Start section."
  },
  {
    "objectID": "content/lakehouse/data.html",
    "href": "content/lakehouse/data.html",
    "title": "Data Ontology",
    "section": "",
    "text": "Initially we will go live with:\n\nBitcoin\nAvalanche\nStacks\n\nFollowed by\n\nSoroban (Stellar)\nMovement Labs (Move based chains)"
  },
  {
    "objectID": "content/lakehouse/data.html#blocks",
    "href": "content/lakehouse/data.html#blocks",
    "title": "Data Ontology",
    "section": "Blocks",
    "text": "Blocks\n\n\n\nColumn name\nColumn type\nDescription\n\n\n\n\nbits\nvarchar\nThe difficulty threshold specified in block header\n\n\nchainwork\n\nThe expected number of hashes required to produce the current chain\n\n\ncoinbase\nvarchar\nThe data specified in the coinbase transaction of the block\n\n\nhash\nvarchar\nThe block hash\n\n\nmerkle_root\nvarchar\nThe root node of a Merkle tree, where leaves are transaction hashes\n\n\nprevious_block_hash\n\nThe hash of the previous block\n\n\ndate\n\nThe block date\n\n\ndifficulty\n\nThe estimated amount of work done to find this block relative to the estimated amount of work done to find block 0\n\n\nheight\nbigint\nThe block number\n\n\nmint_reward\n\nThe output paid out to the miner for minting the block\n\n\nnonce\nvarchar\nThe number of transactions made by the sender prior to this one\n\n\nsize\nbigint\nThe size of the block\n\n\nstripped_size\nbigint\nThe size of the block excluding witness data\n\n\ntime\n\nThe block time\n\n\ntotal_fees\n\nThe fees paid out to the miner from transaction users. Each transaction’s fee is what’s left of output after input is subtracted from it.\n\n\ntotal_reward\n\nThe static reward given to the miner. It is the sum of the outputs in the coinbase transaction (the first transaction).\n\n\ntransaction_count\nbigint\nThe number of transactions in the block\n\n\nversion\nbigint\n\n\n\nweight\nbigint\nThe block weight as defined in BIP 141"
  },
  {
    "objectID": "content/lakehouse/data.html#transactions",
    "href": "content/lakehouse/data.html#transactions",
    "title": "Data Ontology",
    "section": "Transactions",
    "text": "Transactions\n\n\n\n\n\n\n\n\nColumn name\nType\nDescription\n\n\n\n\nblock_time\n\nThe block time\n\n\nblock_date\n\nThe block date\n\n\nblock_height\n\nThe block number\n\n\nblock_hash\n\nThe hash of the block that contains this transaction\n\n\nindex\n\nThe number of the transaction in the block.\n\n\nid\n\nThe id (hash) of this transaction\n\n\ninput_value\n\nTotal value of inputs in the transaction\n\n\noutput_value\n\nTotal value of outputs in the transaction\n\n\nfee\n\nThe transaction fee paid to the miner. = output_value - input_value\n\n\ninput_count\n\nThe number of inputs in the transaction\n\n\noutput_count\n\nThe number of outputs in the transaction\n\n\nsize\n\nThe size of this transaction in bytes\n\n\nvirtual_size\n\nThe virtual transaction size (differs from size for witness transactions)\n\n\nis_coinbase\n\nThe transaction is a coinbase transaction, which is the first transaction in a block\n\n\ncoinbase\n\nIf the transaction is a coinbase transaction, contains the coinbase data. Otherwise, null.\n\n\ninputs\n\nTransaction inputs\n\n\noutputs\n\nTransaction outputs. See outputs table.\n\n\nlock_time\n\nEarliest time that miners can include the transaction in their hashing of the Merkle root to attach it in the latest block of the blockchain\n\n\nhex\n\nThe transaction encoded as hexadecimal\n\n\n\n\nInputs\n\n\n\n\n\n\n\n\nField\nData type\nDescription\n\n\n\n\nvalue\n\nThe number of Satoshis attached to this output\n\n\nheight\n\nThe height of the output\n\n\ntx_id\n\nThe transaction id of the output that is here used as input\n\n\noutput_number\n\nThe number (index) of the output in transaction tx_id’s outputs\n\n\ncoinbase\n\nThe data specified in this transaction, if it was a coinbase transaction\n\n\nsequence\n\nSequence number\n\n\nwitness_data\n\nArray of hex encoded witness data\n\n\nscript_signature\n\nThe script signature\n\n\nscript_pub_key\n\nThe script public key\n\n\n\n\nScript signature\n\n\n\n\n\n\n\n\nField\nData type\nDescription\n\n\n\n\nhex\n\nThe transaction’s script operations, in hex\n\n\nasm\n\nThe transaction’s script operations, in symbolic representation\n\n\n\n\n\nScript public key\n\n\n\n\n\n\n\n\nField\nData type\nDescription\n\n\n\n\nasm\n\nThe transaction’s script operations, in symbolic representation\n\n\ndesc\n\nThe transaction’s script operations, in symbolic representation\n\n\naddress\n\nThe transaction’s script operations, in symbolic representation\n\n\nhex\n\nThe transaction’s script operations, in hex\n\n\ntype\n\nThe address type of the output\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nField\nData type\nDescription\n\n\n\n\nindex\n\n0-indexed number of an output within a transaction used by a later transaction to refer to that specific output\n\n\nvalue\n\nThe number of Satoshis attached to this output\n\n\nscript_pub_key\n\nThe public key\n\n\n\n\nScript public key\n\n\n\n\n\n\n\n\nField\nData type\nDescription\n\n\n\n\nasm\n\nThe transaction’s script operations, in symbolic representation\n\n\nhex\n\nThe transaction’s script operations, in hex\n\n\naddress\n\nThe address the BTC came from\n\n\ntype\n\nThe address type of the output"
  },
  {
    "objectID": "content/lakehouse/data.html#blocks-1",
    "href": "content/lakehouse/data.html#blocks-1",
    "title": "Data Ontology",
    "section": "Blocks",
    "text": "Blocks\n\n\n\nColumn name\nData type\nDescription\n\n\n\n\nbase_fee_per_gas\nbigint\nThis block’s base fee (introduced by EIP1559)\n\n\ndifficulty\nbigint\nThe effort required to mine the block\n\n\nextra_data\nvarchar\n\n\n\ngas_limit\nbigint\nThe gas limit of the current block\n\n\ngas_used\nbigint\nThe gas used in this block\n\n\nhash\nvarchar\nA unique identifier for that block\n\n\nlogs_bloom\nvarchar\n\n\n\nminer\nvarchar\nThe address of the miner\n\n\nnonce\nvarchar\nThe block nonce is used to demonstrate the proof of work during mining\n\n\nnumber\nbigint\nThe length of the blockchain in blocks\n\n\nparent_hash\nvarchar\nThe unique identifier for the prior block\n\n\nreceipts_root\nvarchar\n\n\n\nsha3_uncles\nvarchar\n\n\n\nsize\nbigint\nThis block’s size in bytes (limited by gas limit)\n\n\nstate_root\nvarchar\n\n\n\ntimestamp\nbigint\n\n\n\ntotal_difficulty\nbigint\nTotal difficulty of the chain until this block\n\n\ntransactions_root\nvarchar\n\n\n\ntransactions_count\nbigint"
  },
  {
    "objectID": "content/lakehouse/data.html#transactions-1",
    "href": "content/lakehouse/data.html#transactions-1",
    "title": "Data Ontology",
    "section": "Transactions",
    "text": "Transactions\nTransactions are cryptographically signed instructions from accounts. An account will initiate a transaction to update the state of the Ethereum network. Transactions will always originate from externally owned accounts, a smart contract can not initiate a transaction.\nTransactions need to be broadcast to the whole network. Any node can broadcast a request for a transaction to be executed on the EVM; after this happens, a miner will execute the transaction and propagate the resulting state change to the rest of the network.\nRead more in the official Ethereum documentation here.\n\n\n\nColumn\nData type\nDescription\n\n\n\n\nblock_hash\nvarchar\nA unique identifier for that block\n\n\nblock_number\nbigint\nThe length of the blockchain in blocks\n\n\nblock_timestamp\nbigint\n\n\n\nfrom_address\nvarchar\nAddress of the sender\n\n\ngas\nbigint\nThe gas consumed by the transaction in wei\n\n\ngas_price\nbigint\nThe gas price in wei\n\n\nhash\nvarchar\nA unique identifier for that block\n\n\ninput\nvarchar\n\n\n\nmax_fee_per_gas\nbigint\nThe maximum fee per gas the transaction sender is willing to pay total (introduced by EIP1559)\n\n\nmax_priority_fee_per_gas\nbigint\nMaximum fee per gas the transaction sender is willing to give to miners to incentivize them to include their transaction (introduced by EIP1559)\n\n\nnonce\nvarchar\nThe transaction nonce, unique to that wallet\n\n\nto_address\nvarchar\nAddress of the receiver. null when its a contract creation transaction\n\n\ntransaction_index\nbigint\nThe transactions index position in the block\n\n\ntransaction_type\nbigint\nThe type of the transaction: Legacy, AccessList, or DynamicFee\n\n\nvalue\nbigint\nThe amount of [chain_gas_token] sent in this transaction in wei. Note that ERC20 tokens do not show up here"
  },
  {
    "objectID": "content/lakehouse/data.html#receipts",
    "href": "content/lakehouse/data.html#receipts",
    "title": "Data Ontology",
    "section": "Receipts",
    "text": "Receipts\nComing soon"
  },
  {
    "objectID": "content/lakehouse/data.html#logs",
    "href": "content/lakehouse/data.html#logs",
    "title": "Data Ontology",
    "section": "Logs",
    "text": "Logs\nEvent Logs tables store all logs data that gets generated by smart contracts.\nThis can be useful for querying contracts that are not yet decoded or are not able to be decoded since the code of the smart contract is not public.\nLogs are an elegant way to store tiny amounts of data on EVM blockchains for a small amount of gas. Specifically, event logs are useful to let other people know something has happened without them having to query contracts individually.\nFor more on this topic read this article.\n\n\n\n\n\n\n\n\nColumn name\nData type\nDescription\n\n\n\n\naddress\nvarchar\nThe address of the contract that emitted the log\n\n\nblock_hash\nvarchar\nA unique identifier for that block\n\n\nblock_number\nbigint\nThe length of the blockchain in blocks\n\n\ndata\nvarchar\nUnindexed data containing further information on the event\n\n\nlog_index\nbigint\nThis logs index position in the block (cumulative amount of logs ordered by execution)\n\n\ntopics\nvarchar\n\n\n\ntransaction_hash\nvarchar\nThe transaction hash of the transaction that produced this log\n\n\ntransaction_index\nbigint\nThe index position of the transaction in this block (cumulative amount of transactions ordered by execution)"
  },
  {
    "objectID": "content/lakehouse/data.html#blocks-2",
    "href": "content/lakehouse/data.html#blocks-2",
    "title": "Data Ontology",
    "section": "Blocks",
    "text": "Blocks\n\n\n\nColumn name\nType\nDescription\n\n\n\n\ncanonical\n\n\n\n\nheight\n\n\n\n\nhash\n\n\n\n\nindex_block_hash\n\n\n\n\nparent_block_hash\n\n\n\n\nparent_index_block_hash\n\n\n\n\nburn_block_time\n\n\n\n\nburn_block_time_iso\n\n\n\n\nburn_block_hash\n\n\n\n\nburn_block_height\n\n\n\n\nminer_txid\n\n\n\n\ntx_count\n\n\n\n\nexecution_cost_read_count\n\n\n\n\nexecution_cost_read_length\n\n\n\n\nexecution_cost_runtime\n\n\n\n\nexecution_cost_write_count\n\n\n\n\nexecution_cost_write_length"
  },
  {
    "objectID": "content/lakehouse/data.html#transactions-2",
    "href": "content/lakehouse/data.html#transactions-2",
    "title": "Data Ontology",
    "section": "Transactions",
    "text": "Transactions\n\n\n\nColumn name\nType\nDescription\n\n\n\n\ntx_id\n\n\n\n\nnonce\n\n\n\n\nfee_rate\n\n\n\n\nsender_address\n\n\n\n\nsponsored\n\n\n\n\npost_condition_mode\n\n\n\n\npost_conditions\n\n\n\n\nanchor_mode\n\n\n\n\nin_unanchored\n\n\n\n\nblock_hash\n\n\n\n\nparent_block_hash\n\n\n\n\nblock_height\n\n\n\n\nburn_block_time\n\n\n\n\nburn_block_time_iso\n\n\n\n\ncanonical\n\n\n\n\ntx_index\n\n\n\n\ntx_status\n\n\n\n\ntx_result\n\n\n\n\nmicroblock_hash\n\n\n\n\nmicroblock_sequence\n\n\n\n\nmicroblock_canonical\n\n\n\n\nevent_count\n\n\n\n\nevents\n\n\n\n\nexecution_cost_read_count\n\n\n\n\nexecution_cost_read_length\n\n\n\n\nexecution_cost_runtime\n\n\n\n\nexecution_cost_write_count\n\n\n\n\nexecution_cost_write_length\n\n\n\n\ntx_type\n\n\n\n\ncontract_call"
  },
  {
    "objectID": "content/lakehouse/data.html#contracts",
    "href": "content/lakehouse/data.html#contracts",
    "title": "Data Ontology",
    "section": "Contracts",
    "text": "Contracts\n\n\n\nColumn name\nType\nDescription\n\n\n\n\ntx_id\n\n\n\n\ncanonical\n\n\n\n\ncontract_id\n\n\n\n\nblock_height\n\n\n\n\nclarity_version\n\n\n\n\nsource_code\n\n\n\n\nabi"
  },
  {
    "objectID": "content/lakehouse/data.html#aptos-derived",
    "href": "content/lakehouse/data.html#aptos-derived",
    "title": "Data Ontology",
    "section": "Aptos derived",
    "text": "Aptos derived"
  },
  {
    "objectID": "content/lakehouse/data.html#sui-derived",
    "href": "content/lakehouse/data.html#sui-derived",
    "title": "Data Ontology",
    "section": "Sui derived",
    "text": "Sui derived"
  },
  {
    "objectID": "studio/sqllab.html",
    "href": "studio/sqllab.html",
    "title": "SQL Lab",
    "section": "",
    "text": "SQL Lab\nSQL Lab is a feature-rich SQL editor within Ortege Studio designed for crafting and running complex queries. The interface is user-friendly and supports a variety of functions to enhance your data exploration experience. Let’s take a detailed tour of its interface.\n\nSQL Editor\nScreenshot needed: SQL Editor interface\nThe SQL Editor is the heart of SQL Lab. It’s where you write, edit, and run your SQL queries.\n\nQuery Tab: Located at the top, each tab represents a separate SQL query, allowing you to work on multiple queries simultaneously.\nSQL Editor Pane: This is the large text area where you write your SQL code. It features syntax highlighting and auto-completion to assist with writing queries efficiently.\nRun Button: Click this button to execute the query written in the active tab.\n\n\n\nResults Pane\nScreenshot needed: Results pane showing query results\nBelow the SQL Editor, you’ll find the Results Pane.\n\nData View: Once a query is run, the results are displayed here in a tabular format.\nRow Limit: The interface allows you to specify the number of rows to display, which is useful for large result sets.\nCSV Export: You can export the result of your query directly to a CSV file from this pane.\n\n\n\nData Preview\nScreenshot needed: Data preview of a selected table\n\nData Source Selector: To the left of the SQL Editor, you’ll find the Data Source Selector. Here, you can choose the database and schema you wish to query.\nTable Selector and Preview: Once a table is selected, a preview of the table’s data is displayed, along with column names and data types. This is particularly helpful for understanding the structure of your data before writing a query.\n\n\n\nQuery History\nScreenshot needed: Query history section\n\nAccessing History: The Query History section, accessible from the bottom left of the interface, keeps a log of all executed queries.\nDetails and Actions: Each entry in the history shows the query, the time of execution, and its status. You can also rerun or copy queries from this history.\n\n\n\nAdditional Features\n\nSave Query: You can save queries for future use. This option is located next to the Run button.\nSchema Browser: The Schema Browser, adjacent to the Data Source Selector, lists all tables in the selected schema and provides information about table columns, indexes, and partitions.\n\n\n\nConclusion\nThe SQL Lab interface in Ortege Studio is designed to be intuitive yet powerful, catering to both beginners and experienced SQL users. The seamless integration of query writing, data preview, and result analysis within a single interface makes SQL Lab an efficient tool for data exploration and analysis."
  },
  {
    "objectID": "studio/datasets.html",
    "href": "studio/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets\nDatasets in Ortege Studio are foundational elements that represent the structured data you work with. They are usually tables or views from connected databases. Understanding the Datasets feature is key to effectively utilizing Ortege Studio for your data analysis and visualization needs.\n\nAccessing Datasets\nScreenshot needed: Accessing Datasets from the Ortege Studio menu\n\nNavigation: You can access the Datasets feature from the main Ortege Studio menu, typically found under the “Data” section.\nDatasets Page: This page lists all available datasets, showing their names, database connections, and other relevant details.\n\n\n\nDataset Overview\nScreenshot needed: Overview of a Dataset\n\nMetadata: Each dataset listing provides metadata, such as the database it belongs to, the schema, and when it was last updated.\nActions: From here, you can perform actions like editing dataset properties, deleting datasets, or exploring the dataset through visualizations.\n\n\n\nEditing a Dataset\nScreenshot needed: Dataset editing interface\n\nEditing Interface: By selecting a dataset, you enter the editing interface. This area allows you to modify various aspects of the dataset.\nChanging Metadata: You can change the dataset’s descriptive information, like its verbose name, description, and more.\n\n\n\nColumns and Metrics\nScreenshot needed: Columns and Metrics section of a dataset\n\nColumns Tab: This tab shows all the columns of your dataset, including their type and source. You can edit column details, like renaming them, changing data types, or adding calculated columns.\nMetrics Tab: Here, you can define and manage custom metrics that are specific to the dataset, like aggregations or formulas.\n\n\n\nDataset Usage\nScreenshot needed: Visualization using a selected dataset\n\nVisualization: Datasets can be directly used to create various visualizations and dashboards.\nSQL Lab Integration: Datasets also integrate seamlessly with SQL Lab, allowing for advanced querying and exploration.\n\n\n\nSecurity and Permissions\nScreenshot needed: Dataset’s security settings\n\nAccess Control: Ortege Studio enables fine-grained access control on datasets, allowing you to control who can view or edit them.\nRow-Level Security: You can also set up row-level security to ensure users only see data they are permitted to view.\n\n\n\nConclusion\nThe Datasets feature in Ortege Studio is a versatile and crucial component of the platform. It allows for the organized management and utilization of your data within Ortege Studio, facilitating everything from simple explorations to complex visualizations and analyses."
  }
]