[
  {
    "objectID": "studio/charts.html",
    "href": "studio/charts.html",
    "title": "Charts",
    "section": "",
    "text": "Charts\nThe Charts feature in Ortege Studio is a powerful tool for creating and managing a wide range of data visualizations. From simple line charts to complex geospatial visualizations, Charts enable users to turn data into insightful and interactive visual representations.\n\nAccessing Charts\nScreenshot needed: Accessing Charts from the Studio menu\n\nNavigation: Charts can be accessed from the main Ortege Studio menu, typically under the “Charts” section.\nCharts Page: This page lists all the created charts, showing details like the chart name, type, and the dataset it’s based on.\n\n\n\nCreating a New Chart\nScreenshot needed: Creating a new chart interface\n\nChart Creation: To create a new chart, click on the “+” or “New” button.\nChoosing a Dataset: First, select the dataset you want to visualize.\nSelecting Chart Type: Ortege Studio offers a variety of chart types. Choose one that best suits the data and the insights you wish to convey.\n\n\n\nChart Customization\nScreenshot needed: Chart customization interface\n\nCustomization Panel: Once you’ve selected the chart type, you’ll be taken to the customization panel. Here, you can define and tweak various aspects of your chart.\nSetting Metrics and Dimensions: Choose the metrics and dimensions from your dataset that you want to visualize.\nChart Options: Adjust settings such as filters, sorting, and chart-specific options to fine-tune your visualization.\n\n\n\nVisualizing the Data\nScreenshot needed: Finished chart visualization\n\nPreviewing the Chart: As you customize your chart, you can preview it in real-time. This feature helps in iteratively building your visualization.\nInteractivity: Many chart types in Ortege Studio are interactive, allowing users to hover over elements to see more details or click to drill down further.\n\n\n\nSaving and Sharing Charts\nScreenshot needed: Saving and sharing options for a chart\n\nSaving the Chart: Once satisfied with the visualization, you can save the chart, giving it a descriptive name and, if necessary, adding it to a dashboard.\nSharing: Ortege Studio allows you to share charts with others, either through direct links or by adding them to shared dashboards.\n\n\n\nAdvanced Features\nScreenshot needed: Advanced features in chart creation, like SQL editing\n\nSQL Editing: For advanced users, Ortege Studio allows editing the underlying SQL of a chart, providing more control over the data and its presentation.\nCustom Visualization Plugins: Users can extend Ortege Studio’s visualization capabilities by adding custom chart plugins.\n\n\n\nConclusion\nThe Charts feature in Ortege Studio is a cornerstone of its data visualization capabilities. It offers a broad spectrum of options to represent data graphically, catering to both novice users and data experts. With its intuitive interface and wide range of customization options, Charts empower users to transform data into actionable insights."
  },
  {
    "objectID": "studio/dashboards.html",
    "href": "studio/dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "Dashboards\nDashboards in Ortege Studio are powerful tools for combining various charts and visualizations into a single, interactive, and dynamic view. They provide a holistic view of your data, allowing for efficient monitoring and insightful decision-making.\n\nAccessing Dashboards\nScreenshot needed: Accessing Dashboards from the Ortege Studio menu\n\nNavigation: You can find Dashboards in the main Ortege Studio menu, often under a section labeled “Dashboards.”\nDashboard Page: This page displays all the available dashboards, with details like the dashboard name, owner, and last modified date.\n\n\n\nCreating a New Dashboard\nScreenshot needed: Creating a new dashboard interface\n\nStarting a New Dashboard: To create a new dashboard, click on the “+” or “New” button.\nNaming the Dashboard: Assign a name and optionally a description to your new dashboard for easy identification.\n\n\n\nAdding Charts to Dashboard\nScreenshot needed: Adding charts to a dashboard\n\nChart Selection: In the dashboard edit mode, you can add charts that you’ve previously created in the Charts section.\nPositioning and Resizing: Drag and drop the charts into your desired position on the dashboard. You can also resize them for optimal layout and visibility.\n\n\n\nCustomizing the Dashboard\nScreenshot needed: Customizing the dashboard layout and settings\n\nLayout Customization: You can customize the layout of the dashboard, organizing charts and components in a way that best tells your data story.\nFilter Options: Add interactive filters to the dashboard that allow users to dynamically change the data displayed across multiple charts.\n\n\n\nInteracting with Dashboards\nScreenshot needed: Interacting with a live dashboard\n\nData Interaction: Users can interact with the data in various ways, like hovering over charts to see detailed information or clicking on elements to drill down.\nFiltering: Apply filters to refine the data displayed on the entire dashboard or specific charts.\n\n\n\nSharing and Exporting\nScreenshot needed: Sharing and exporting options for a dashboard\n\nSharing Dashboards: Dashboards can be shared with others through direct links or by providing access within Superset.\nExporting: Dashboards can also be exported for external use, either as images or in other formats, depending on Ortege Studio’s configuration.\n\n\n\nSecurity and Permissions\nScreenshot needed: Dashboard’s security settings\n\nAccess Control: You can control who has access to view or edit the dashboard, ensuring data security and integrity.\nRow-Level Security: If set up, row-level security will apply to all the charts in the dashboard, ensuring users only see the data they are authorized to view.\n\n\n\nConclusion\nDashboards in Ortege Studio are an essential feature for synthesizing complex data into comprehensible and actionable insights. They offer a platform to narrate a data story, combining different visual elements into a cohesive and interactive experience."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Ortege",
    "section": "",
    "text": "Ortege Studio\nOrtege Studio: Ortege Studio is a cutting-edge analytics platform designed to empower users with unparalleled insights into their data through an extensive array of visualization options, with over 30 types of charts available to cater to a wide range of analytical needs. At its core, Ortege Studio harnesses the power of predictive analytics, enabling businesses to forecast trends and make data-driven decisions with greater accuracy. The platform stands out by offering the flexibility to connect to a multitude of data sources, allowing users to seamlessly integrate and harmonize disparate data sets. Thanks to its sophisticated Data Manipulation Language (DML) capabilities, users can effortlessly join, query, and analyze their data across various sources, all within a user-friendly interface. Ortege Studio is the ultimate tool for organizations looking to unlock the full potential of their data, offering a comprehensive solution that blends advanced analytics with intuitive data integration and exploration features.\n\n\nOrtege Lakehouse\nOrtege Lakehouse: Ortege Lakehouse is an innovative data management platform engineered for the era of big data, embodying cutting-edge technology to meet the demands of extensive datasets and complex analytics. Designed to effortlessly integrate with MLOps frameworks and Large Language Models (LLMs), it provides a robust foundation for advanced machine learning projects and predictive analytics. A standout feature of Ortege Lakehouse is its comprehensive datasets for a wide array of the market’s most popular blockchains. With these datasets, users gain immediate access to a wealth of blockchain data, streamlined for analysis. Moreover, our platform introduces “silver and gold layers” – a feature that simplifies the querying of complex blockchain datasets, ensuring that even the most intricate data becomes easily navigable. This makes Ortege Lakehouse an indispensable tool for organizations looking to leverage blockchain data for insightful analysis, predictive modeling, and strategic decision-making, all while ensuring seamless workflow integration and operational efficiency.\n\n\nOrtegeGPT\nOrtegeGPT: OrtegeGPT, our latest innovation currently under rigorous development, is poised to redefine the way we interact with blockchain data. This cutting-edge Large Language Model (LLM) is being meticulously crafted to provide real-time, intuitive querying capabilities across a vast spectrum of supported blockchains. Upon completion, OrtegeGPT will empower users to effortlessly extract insights, perform complex analyses, and obtain answers to virtually any query about the blockchain ecosystems we support. Recognizing the diverse needs of our users, OrtegeGPT is also designed with flexibility in mind, allowing organizations to integrate their proprietary data seamlessly, thereby enriching their analytical capabilities and insights. Furthermore, OrtegeGPT is set to offer multiple distribution channels, ensuring that our advanced LLM can be accessed conveniently, whether through direct API integrations, web interfaces, or customized applications. This ambitious feature signifies our commitment to pioneering in the blockchain analytics space, offering an unparalleled tool that combines the latest in AI technology with the expansive world of blockchain data."
  },
  {
    "objectID": "content/studio/dbconnections.html",
    "href": "content/studio/dbconnections.html",
    "title": "Database Connections",
    "section": "",
    "text": "Database Connections in Ortege Studio\nDatabase connections are a fundamental aspect of Ortege Studio, enabling users to access and interact with data stored across various database systems. This feature provides the foundation for all data analysis, visualization, and dashboard creation within Ortege Studio. This section of the documentation will guide you through the process of setting up, managing, and troubleshooting database connections.\n\n\nSetting Up Database Connections\n\nAccess Connection Settings: Navigate to the database connections section within Ortege Studio’s settings. Here, you’ll find options to add a new connection or modify existing ones.\nProvide Connection Details: To connect to a database, you’ll need to provide specific details, such as the database type (e.g., MySQL, PostgreSQL, Oracle), connection name, and necessary credentials (username, password, server address, port, and database name). Ortege Studio supports a wide range of databases, catering to diverse data storage needs.\nTest and Save: Before finalizing the connection, use the “Test Connection” feature to verify that Ortege Studio can communicate with your database. If successful, save the connection for future use.\n\n\n\nManaging Database Connections\n\nConnection Pooling: Ortege Studio allows you to configure connection pooling, which maintains a pool of database connections that can be reused, improving the efficiency of data operations.\nSecurity and Access Control: Manage who can access each database connection within Ortege Studio. Ensure that only authorized users have access to sensitive data by configuring user permissions and roles.\nUpdating Connections: As your database configurations change, you may need to update connection details. Ortege Studio makes it easy to edit existing connections to reflect new credentials or parameters.\n\n\n\nAdvanced Configuration Options\n\nSSL Encryption: For enhanced security, Ortege Studio supports SSL encryption for database connections. This ensures that data transmitted between Ortege Studio and your database is encrypted and secure.\nSSH Tunneling: If direct connections to your database are not possible due to network restrictions, Ortege Studio supports SSH tunneling as a secure method to connect through an intermediary server.\nQuery Performance: Some databases allow for additional parameters to optimize query performance, such as specifying fetch size or enabling query caching. Explore these options to enhance your data analysis experience.\n\n\n\nBest Practices for Database Connections\n\nRegularly Review Connections: Periodically review and test your database connections to ensure they are functioning correctly and reflect the current database configurations.\nSecure Your Credentials: Use secure methods to store and manage database credentials, avoiding hard-coded credentials within scripts or documents.\nMonitor Usage and Performance: Keep an eye on database performance and usage patterns, especially for connections that support multiple users or high-volume data operations. Adjust pooling settings and query optimization as needed.\n\n\n\nTroubleshooting Common Connection Issues\n\nConnectivity Problems: Verify network settings, firewall rules, and database server availability if you encounter connectivity issues. Ensure that the database server allows connections from Ortege Studio’s IP address.\nAuthentication Failures: Double-check the credentials and authentication methods required by your database. Some databases may require specific configurations or permissions for third-party applications like Ortege Studio.\nTimeouts and Performance: For timeouts or slow query performance, consider optimizing your database indexes, reviewing query efficiency, and adjusting connection or server settings to accommodate higher loads.\n\nDatabase connections are crucial for leveraging the full capabilities of Ortege Studio, providing the gateway to your data. By carefully setting up, managing, and troubleshooting these connections, you ensure a seamless and efficient data analysis workflow within your organization."
  },
  {
    "objectID": "content/studio/dashboards.html",
    "href": "content/studio/dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "Dashboards in Ortege Studio\nDashboards are powerful tools within Ortege Studio that allow users to compile and visualize various charts and widgets into a single, cohesive interface. Designed for flexibility and interactivity, dashboards provide a snapshot of key metrics, trends, and insights, enabling data-driven decision-making at a glance. This section of the documentation will guide you through creating, customizing, and sharing dashboards in Ortege Studio.\n\n\nCreating a Dashboard\n\nInitiate Dashboard Creation: Navigate to the Dashboard section and click on the “Create” button. You’ll be prompted to choose between starting from scratch or duplicating an existing dashboard.\nAdd Charts: Once your dashboard shell is created, add charts by clicking the “+ Add Chart” button. You can select from existing charts or create new ones to add to the dashboard.\nArrange Layout: Drag and drop charts to arrange them on your dashboard. You can adjust the size and position of each chart to create a visually appealing layout.\n\n\n\nCustomizing Dashboards\n\nEdit Properties: Access the dashboard’s properties to rename it, add a description, or update its settings. This is where you can make the dashboard public or private and set refresh intervals.\nApply Filters: Enhance dashboard interactivity by adding filters. Filters allow viewers to refine what data is displayed across the entire dashboard, making it more dynamic and useful.\nStyling: Use the styling options to apply your branding or preferred visual theme to the dashboard. You can adjust colors, fonts, and spacing to match your organization’s style guide.\n\n\n\nSharing Dashboards\n\nDirect Sharing: Share a dashboard directly with other Ortege Studio users by granting them access. You can control whether they can view, edit, or manage the dashboard.\nPublic Link: Generate a public link to your dashboard for easy sharing with external stakeholders. Ensure sensitive data is appropriately protected when using this feature.\nExporting: Dashboards can be exported as images or PDF documents for offline viewing or inclusion in reports and presentations.\n\n\n\nBest Practices for Dashboard Design\n\nFocus on Clarity: Ensure your dashboards are easy to understand at a glance. Use clear chart titles and labels, and avoid overcrowding the dashboard with too many charts.\nLogical Grouping: Group related charts close to each other to tell a cohesive story. This helps users quickly make connections between different data points.\nConsistent Styling: Apply a consistent style across all charts for a professional appearance. Consistency in colors, fonts, and chart types improves readability.\n\n\n\nMaintaining Dashboards\n\nRegular Updates: Keep your dashboards relevant by regularly updating the charts and data they display. Consider setting up automatic data refreshes if supported.\nGather Feedback: Engage with your dashboard viewers to collect feedback and identify areas for improvement. User input can guide you in refining and enhancing your dashboards.\n\nDashboards are a central feature of Ortege Studio, offering a powerful way to present and explore data. By following these guidelines, you can create effective and impactful dashboards that serve as valuable tools for your organization’s data analysis and decision-making processes."
  },
  {
    "objectID": "content/studio/overview.html",
    "href": "content/studio/overview.html",
    "title": "Ortege Studio Overview",
    "section": "",
    "text": "Overview of Ortege Studio\nOrtege Studio is a premier analytics platform that empowers users to visualize, explore, and share data insights in a highly interactive and user-friendly environment. Built for versatility and performance, Ortege Studio facilitates the creation of dynamic dashboards and charts, management of extensive datasets, and execution of complex SQL queries. Its seamless integration capabilities with various database connections make it an ideal solution for organizations looking to harness the power of their data. Whether you’re a data analyst, business intelligence professional, or data scientist, Ortege Studio provides the tools you need to transform data into actionable insights.\n\n\nDashboards\nDashboards in Ortege Studio serve as a centralized space where users can create, view, and interact with a collection of related charts and visualizations. These dashboards are customizable, allowing users to tailor the layout and appearance to meet their specific reporting needs. Users can easily share dashboards with stakeholders, providing a snapshot of key metrics and trends at a glance.\n\n\nCharts\nThe Charts section of Ortege Studio is where data comes to life. Users can choose from an extensive library of chart types to best represent their data, from simple line charts to complex scatter plots. This section guides users through the process of creating, customizing, and refining charts, including selecting data sources, applying filters, and adjusting visual aesthetics to highlight critical insights.\n\n\nDatasets\nDatasets are the foundation of any analysis in Ortege Studio. This section explains how to manage and prepare datasets for analysis, including importing data from various sources, creating custom metrics and dimensions, and optimizing data for performance. Users will learn how to leverage datasets to fuel their charts and dashboards, ensuring accurate and up-to-date information is always at their fingertips.\n\n\nSQL Lab\nSQL Lab is Ortege Studio’s interactive environment for crafting and executing SQL queries. It’s designed for users who prefer direct manipulation of their data through SQL. This powerful tool supports query creation, visualization, and sharing, enabling users to explore their data in-depth. The documentation will cover SQL Lab’s features, such as query history, saved queries, and the ability to preview and export results.\n\n\nDatabase Connections\nThe Database Connections section details how to connect Ortege Studio to a variety of data sources, ensuring users can access and analyze data stored in different databases and platforms. It includes step-by-step instructions for establishing connections, managing database credentials securely, and troubleshooting common connection issues. This section is crucial for organizations looking to integrate Ortege Studio into their existing data infrastructure seamlessly."
  },
  {
    "objectID": "content/lakehouse/overview.html",
    "href": "content/lakehouse/overview.html",
    "title": "Overview of Ortege Lakehouse",
    "section": "",
    "text": "Ortege Lakehouse represents a cutting-edge data management and analytics platform, meticulously designed to accommodate the vast and growing needs of blockchain data analysis. At the core of our innovation is a seamless integration with OrtegeETL, comprehensive access through Ortege Studio, and exclusive API access for institutional customers. This platform not only underscores our commitment to leveraging open-source software but also ensures that our users have unparalleled access to blockchain datasets."
  },
  {
    "objectID": "content/lakehouse/overview.html#bronze-layer-the-foundation-with-raw-data",
    "href": "content/lakehouse/overview.html#bronze-layer-the-foundation-with-raw-data",
    "title": "Overview of Ortege Lakehouse",
    "section": "Bronze Layer: The Foundation with Raw Data",
    "text": "Bronze Layer: The Foundation with Raw Data\nThe Bronze Layer forms the foundational bedrock of Ortege Lakehouse, containing the raw, unprocessed data directly ingested from multiple blockchains and multiple external sources. This layer captures a wide array of blockchain transactions and metadata, preserving the original integrity and granularity of the data. Ideal for users who need access to the most unadulterated form of blockchain data, the Bronze Layer offers a comprehensive snapshot of blockchain ecosystems in their native state."
  },
  {
    "objectID": "content/lakehouse/overview.html#silver-layer-enhanced-readability-and-transformation",
    "href": "content/lakehouse/overview.html#silver-layer-enhanced-readability-and-transformation",
    "title": "Overview of Ortege Lakehouse",
    "section": "Silver Layer: Enhanced Readability and Transformation",
    "text": "Silver Layer: Enhanced Readability and Transformation\nBuilding upon the raw data stored in the Bronze Layer, the Silver Layer introduces a series of transformations and enhancements to make the data more accessible and human-readable. This layer focuses on cleaning, structuring, and enriching the raw data, applying consistent schemas, and resolving common issues such as data normalization and deduplication. The Silver Layer is designed for users who require a balance between the raw detail of blockchain transactions and the convenience of pre-processed data, enabling more straightforward analysis and exploration."
  },
  {
    "objectID": "content/lakehouse/overview.html#gold-layer-advanced-analytics-and-specialized-datasets",
    "href": "content/lakehouse/overview.html#gold-layer-advanced-analytics-and-specialized-datasets",
    "title": "Overview of Ortege Lakehouse",
    "section": "Gold Layer: Advanced Analytics and Specialized Datasets",
    "text": "Gold Layer: Advanced Analytics and Specialized Datasets\nThe Gold Layer stands at the forefront of Ortege Lakehouse’s Medallion Layer Architecture, delivering unparalleled data sophistication and utility. In this layer, we integrate advanced machine learning and AI-driven insights, pushing the boundaries of data intelligence within the blockchain realm. It is home to highly complex datasets, including in-depth analyses of STX stacking on the Stacks blockchain and consolidated DeFi activity tables, offering a multi-faceted view of decentralized finance operations. Designed for peak performance, the Gold Layer facilitates quick and incisive access to data that has been meticulously refined for precise use cases. This tier caters to those who demand the most detailed and actionable intelligence, with optimized queries and models that enable users to perform rapid analysis and make informed decisions with confidence."
  },
  {
    "objectID": "content/lakehouse/api.html",
    "href": "content/lakehouse/api.html",
    "title": "API",
    "section": "",
    "text": "Welcome to the Ortege Lakehouse API documentation. Our APIs offer seamless, secure, and efficient access to the Ortege Lakehouse, empowering you to run SQL queries against a vast, curated dataset tailored to meet the evolving needs of your projects. This documentation is designed to guide you through using our APIs to access and manipulate data within the Ortege Lakehouse, ensuring you can harness the full potential of our platform to drive insights and innovation."
  },
  {
    "objectID": "content/lakehouse/api.html#authentication",
    "href": "content/lakehouse/api.html#authentication",
    "title": "API",
    "section": "Authentication",
    "text": "Authentication\nTo begin using the Ortege Lakehouse APIs, you’ll need an API access key. Please reach out to christos@ortege.io in order to obtain an API key.\nOnce you have your access key, you’re ready to make your first API call. Here’s a simple example to get you started:\nCopy code\ncurl -X POST -H \"Authorization: Bearer YOUR_ACCESS_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"sql\": \"SELECT * FROM your_dataset LIMIT 10\"}' \\\n     https://api.ortege.ai/lakehouse/query\nReplace YOUR_ACCESS_KEY with your actual access key and your_dataset with the dataset you wish to query."
  },
  {
    "objectID": "content/lakehouse/api.html#query-data",
    "href": "content/lakehouse/api.html#query-data",
    "title": "API",
    "section": "Query Data",
    "text": "Query Data\n\nEndpoint: /lakehouse/query\nMethod: POST\nDescription: Run SQL queries against datasets in the Ortege Lakehouse.\nRequest Body:\n\nsql: The SQL query string you wish to execute.\n\nResponse: A JSON object containing the query results."
  },
  {
    "objectID": "content/lakehouse/bronze.html",
    "href": "content/lakehouse/bronze.html",
    "title": "Bronze Layer",
    "section": "",
    "text": "The Bronze Layer acts as the primary reservoir for raw data from varied sources. This layer serves as the initial platform for data analysis and processing within the Ortege ecosystem."
  },
  {
    "objectID": "content/lakehouse/bronze.html#general",
    "href": "content/lakehouse/bronze.html#general",
    "title": "Bronze Layer",
    "section": "General",
    "text": "General\nThese tables cover addresses across multiple blockchains and aren’t blockchain specific\n\ntbl_test_br_address_labels\nClassifies blockchain addresses based on their associated activities. Currently, this table includes Bitcoin addresses known for participating in Stacks (STX) token stacking. This table allows users to quickly identify and analyze addresses that are active in specific blockchain operations.\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\n\n\n\n\naddress\nString\nThe unique identifier for a wallet on the blockchain.\n\n\nblockchain\nString\nThe name of the blockchain network, e.g., ‘bitcoin’.\n\n\nlabel\nString\nA descriptive label for the address, e.g., ‘STX stacker’."
  },
  {
    "objectID": "content/lakehouse/bronze.html#bitcoin",
    "href": "content/lakehouse/bronze.html#bitcoin",
    "title": "Bronze Layer",
    "section": "Bitcoin",
    "text": "Bitcoin\nThese tables specifically focus on Bitcoin and interesting wallets in the Bitcoin ecosystem\n\nWallet first seen (tbl_test__br_address_bitcoin_first_seen)\nThis table returns all wallets on Bitcoin as well as the date the address was first seen\n\n\nWallet size (tbl_test_br_bitcoin_wallet_size)\nThis table classifies all Bitcoin addresses by their BTC holdings. We classify wallets as:\n\n&lt; 1 BTC Shrimp\nBetween 1 BTC and &lt; 10 BTC Crap\nBetween 10 BTC and &lt; 100 BTC Fish\nBetween 100 BTC and &lt; 1K BTC Shark\nBetween 1K BTC and &lt; 10k BTC Whale\nMore than 10k BTC Mega Whale"
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_prod_br_prices",
    "href": "content/lakehouse/bronze.html#tbl_prod_br_prices",
    "title": "Bronze Layer",
    "section": "tbl_prod_br_prices",
    "text": "tbl_prod_br_prices\nThis dataset includes various metrics such as supply details, current price, market capitalization, and price change percentages over different time frames. It also contains a timestamp indicating when the data was recorded or updated.\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\n\n\n\n\nid\nInteger/String\nUnique identifier for the cryptocurrency record.\n\n\nname\nString\nThe full name of the cryptocurrency.\n\n\nsymbol\nString\nThe abbreviated symbol or ticker used to represent the cryptocurrency.\n\n\ncirculating_supply\nDecimal/Double\nThe amount of cryptocurrency that is currently circulating in the market and publicly available.\n\n\ntotal_supply\nDecimal/Double\nThe total amount of cryptocurrency that exists right now (minus any coins that have been verifiably burned).\n\n\nprice\nDecimal/Double\nThe current price of the cryptocurrency.\n\n\nmarket_cap\nDecimal/Double\nThe total market value of the cryptocurrency’s circulating supply.\n\n\npercent_change_1h\nDecimal/Double\nThe percentage change in price over the last hour.\n\n\npercent_change_24h\nDecimal/Double\nThe percentage change in price over the last 24 hours.\n\n\npercent_change_7d\nDecimal/Double\nThe percentage change in price over the last 7 days.\n\n\npercent_change_30d\nDecimal/Double\nThe percentage change in price over the last 30 days.\n\n\nvolume_24h\nDecimal/Double\nThe total trading volume of the cryptocurrency over the last 24 hours.\n\n\ntimestamp\nTimestamp\nThe date and time at which the record was updated."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_prod_br_blocks",
    "href": "content/lakehouse/bronze.html#tbl_prod_br_blocks",
    "title": "Bronze Layer",
    "section": "tbl_prod_br_blocks",
    "text": "tbl_prod_br_blocks\nThis table encapsulates comprehensive details on Bitcoin blocks, encompassing mining data, block identification, and transaction specifics. It records the difficulty level, mining rewards, the unique hash identifying each block, its position within the blockchain, and the Merkle root for transaction integrity verification. Additionally, it includes metrics such as the block’s size, the total number of transactions it contains, and the cumulative work or “chainwork” up to that point, expressed in hexadecimal. This table not only provides insights into the mining process, including the nonce and the compact target “bits,” but also captures the economic aspects of mining through fields like mint reward, total fees, and the overall reward. The tbl_dev_br_blocks serves as a pivotal resource for analyzing the blockchain’s structural and economic dynamics, offering a granular view of each block’s contribution to the ledger.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nbits\nstring\nThe encoded form of the target threshold this block’s header hash must be less than or equal to.\n\n\nchainwork\nstring\nThe total amount of work done on this block’s chain, in hexadecimal.\n\n\ncoinbase\nstring\nThe transaction containing the miner’s reward and any fees paid by transactions included in this block.\n\n\ndate\nstring\nThe date and time when this block was mined.\n\n\ndifficulty\nstring\nThe difficulty target for this block.\n\n\nhash\nstring\nThe hash of this block’s header.\n\n\nheight\nbigint\nThe height of this block in the blockchain.\n\n\nmerkle_root\nstring\nThe Merkle root of the transactions included in this block.\n\n\nmint_reward\ndouble\nThe total amount of new bitcoins generated by mining this block.\n\n\nnonce\nstring\nA random number used to try to find a valid block hash.\n\n\nprevious_block_hash\nstring\nThe hash of the previous block in the blockchain.\n\n\nsize\nbigint\nThe size of this block in bytes.\n\n\nstripped_size\nbigint\nThe size of this block without witness data in bytes.\n\n\ntime\ntimestamp\nThe Unix timestamp when this block was mined.\n\n\ntotal_fees\ndouble\nThe total amount of fees paid by transactions included in this block.\n\n\ntotal_reward\ndouble\nThe total amount of bitcoins generated by mining this block, including both the mint reward and transaction fees.\n\n\ntransaction_count\nbigint\nThe number of transactions included in this block.\n\n\nweight\nbigint\nThe weight of this block, used to calculate transaction fees."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_prod_br_transactions",
    "href": "content/lakehouse/bronze.html#tbl_prod_br_transactions",
    "title": "Bronze Layer",
    "section": "tbl_prod_br_transactions",
    "text": "tbl_prod_br_transactions\nThis table provides a detailed ledger of transactions within the Bitcoin chain, capturing both the technical aspects and the economic activities of each transaction. It records the transaction’s metadata, such as its unique identifier (id), the block_hash it belongs to, its block_height and timing (block_date and block_time), and its placement within the block (index). The table delves into the composition of transactions, detailing the number and total value of inputs (input_count and input_value) and outputs (output_count and output_value), alongside arrays of input and output objects for granular transaction analysis. It also highlights whether a transaction is a coinbase transaction, indicating a miner’s reward (is_coinbase), the transaction fees involved (fee), and technical parameters like hex representation, lock_time, size, version, and virtual_size. This comprehensive transactional overview aids in understanding the flow of bitcoins, transaction costs, and the operational dynamics of the Bitcoin blockchain.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nblock_date\ndate\nThe date when the block was added to the blockchain\n\n\nblock_hash\nstring\nThe unique identifier of the block\n\n\nblock_height\nbigint\nThe height of the block in the blockchain\n\n\nblock_time\ntimestamp\nThe time when the block was added to the blockchain\n\n\ncoinbase\nstring\nThe address that received the block reward for mining the block\n\n\nfee\nbigint\nThe transaction fee paid by the sender\n\n\nhex\nstring\nThe hexadecimal representation of the transaction\n\n\nid\nstring\nThe unique identifier of the transaction\n\n\nindex\nbigint\nThe index of the transaction in the block\n\n\ninput_count\nbigint\nThe number of inputs in the transaction\n\n\ninput_value\nbigint\nThe total value of all inputs in the transaction\n\n\ninputs\narray\nAn array of input objects containing information about each input in the transaction\n\n\nis_coinbase\nboolean\nA boolean indicating whether the transaction is a coinbase transaction\n\n\nlock_time\nbigint\nThe lock time of the transaction\n\n\noutput_count\nbigint\nThe number of outputs in the transaction\n\n\noutput_value\nbigint\nThe total value of all outputs in the transaction\n\n\noutputs\narray\nAn array of output objects containing information about each output in the transaction\n\n\nsize\nbigint\nThe size of the transaction in bytes\n\n\nversion\nbigint\nThe version number of the transaction\n\n\nvirtual_size\nbigint\nThe virtual size of the transaction"
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_prod_br_cycles",
    "href": "content/lakehouse/bronze.html#tbl_prod_br_cycles",
    "title": "Bronze Layer",
    "section": "tbl_prod_br_cycles",
    "text": "tbl_prod_br_cycles\nThe tbl_dev_sl_cycles table provides a structured overview of cycles within the Stacks blockchain, starting from the genesis of Stacks 2.0. Each cycle represents a fixed number of blocks, facilitating the organization and analysis of blockchain data over time. This table is essential for understanding the temporal division of blockchain activity and aids in analyzing trends, rewards distributions, and other cycle-based metrics.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ncycle\nThe sequential number of the cycle, starting from 0. Each cycle represents a group of 2100 blocks within the Stacks blockchain.\n\n\nstart_block\nThe starting block number for the given cycle. This marks the first block in the cycle.\n\n\nend_block\nThe ending block number for the given cycle. This marks the last block in the cycle."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_prod_br_blocks-1",
    "href": "content/lakehouse/bronze.html#tbl_prod_br_blocks-1",
    "title": "Bronze Layer",
    "section": "tbl_prod_br_blocks",
    "text": "tbl_prod_br_blocks\nThe table archives block data for the Stacks blockchain, detailing block identification, sequencing, and associated Bitcoin blockchain references due to Stacks’ unique proof-of-burn consensus mechanism. It records block numbers, hashes, transaction counts, and the execution costs related to transactions, offering essential insights for analysis of network activity and transactional efficiency on the Stacks blockchain.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\ncanonical\nboolean\nCanonical Blocks are the building blocks that contribute to the continuous, unbroken chain of blocks in the blockchain, each linking to its predecessor, forming the indelible ledger that blockchain is celebrated for. You expect this to be true.\n\n\nnumber\nbigint\nstacks block number\n\n\nhash\nstring\nA unique identifier for each block, generated by applying a cryptographic hash function to the block’s data\n\n\nindex_block_hash\nstring\nThe hash of the index block.\n\n\nparent_block_hash\nstring\nThe hash of the parent block.\n\n\ntimestamp\nbigint\nTimestamp of relevant block. If you use to_timestamp(timestamp) SQL code, you will get the actual date-time.\n\n\nburn_block_hash\nstring\nCorresponding Bitcoin block number’s hash. The block that is written in Bitcoin block’s hash.\n\n\nburn_block_height\nbigint\nCorresponding Bitcoin block number. The block that is written in a Bitcoin block.\n\n\nminer_txid\nstring\nCorresponding transaction burned on Bitcoin block for the stacks block. Every stacks block is a transaction on Bitcoin. And this is that tx’s id. If you remove the 0x from the beginning, you will get the Bitcoin tx id.\n\n\ntransaction_count\nbigint\nCount of each transaction in the relevant block.\n\n\nexecution_cost_read_count\nbigint\nCaptures the number of independent reads performed on the underlying data store.\n\n\nexecution_cost_read_length\nbigint\nThe number of bytes read from the underlying data store.\n\n\nexecution_cost_runtime\nbigint\nCaptures the number of cycles that a single processor would require to process the Clarity block. This is a unitless metric, meant to provide a basis for comparison between different Clarity code blocks.\n\n\nexecution_cost_write_count\nbigint\nCaptures the number of independent writes performed on the underlying data store (see SIP-004).\n\n\nexecution_cost_write_length\nbigint\nThe number of bytes written to the underlying data store."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_prod_br_transactions-1",
    "href": "content/lakehouse/bronze.html#tbl_prod_br_transactions-1",
    "title": "Bronze Layer",
    "section": "tbl_prod_br_transactions",
    "text": "tbl_prod_br_transactions\nThe table is a comprehensive record of transactions on the Stacks blockchain. It includes transaction identifiers, sender information, execution costs, and status details. This table links transactions to blocks, tracks microblock inclusion, and categorizes transaction types, such as token transfers and smart contract executions. It’s a key resource for analyzing transactional data within the Stacks ecosystem.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nhash\nstring\nUnique identifier for the transaction\n\n\nnonce\nbigint\nRandom number used in mining process\n\n\nfee_rate\nstring\nFee rate for the transaction\n\n\nsender_address\nstring\nAddress of the sender\n\n\nsponsored\nboolean\nBoolean indicating if the transaction is sponsored\n\n\npost_condition_mode\nstring\nMode for post-conditions\n\n\npost_conditions\narray\nArray of post-conditions\n\n\nanchor_mode\nstring\nMode for anchoring\n\n\nis_unanchored\nboolean\nBoolean indicating if the transaction is unanchored\n\n\nparent_block_hash\nstring\nHash of the parent block\n\n\nblock_hash\nstring\nHash of the block containing the transaction\n\n\nblock_number\nbigint\nBlock number containing the transaction\n\n\nblock_timestamp\nbigint\nTimestamp of the block containing the transaction\n\n\nparent_burn_block_time\nbigint\nTimestamp of the parent burn block\n\n\ncanonical\nboolean\nBoolean indicating if the block is canonical\n\n\ntx_index\nbigint\nIndex of the transaction within the block\n\n\ntx_status\nstring\nStatus of the transaction\n\n\ntx_result\nstruct\nResult of the transaction\n\n\nmicroblock_hash\nstring\nHash of the microblock containing the transaction\n\n\nmicroblock_sequence\nbigint\nSequence number of the microblock containing the transaction\n\n\nmicroblock_canonical\nboolean\nBoolean indicating if the microblock is canonical (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nevent_count\nbigint\nCount of events (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nevents\narray\nArray of events (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_read_count\nbigint\nNumber of independent reads performed on the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_read_length\nbigint\nNumber of bytes read from the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_runtime\nbigint\nNumber of cycles for processing (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_write_count\nbigint\nNumber of independent writes performed on the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_write_length\nbigint\nNumber of bytes written to the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ntx_type\nstring\nType of the transaction (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ntoken_transfer\nstruct\nDetails of token transfer if applicable (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ncoinbase_payload\nstruct\nData for coinbase transactions (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nsmart_contract\nstruct\nDetails of the smart contract if applicable (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ncontract_call\nstruct\nDetails of the contract call if applicable (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\npoison_microblock\nmap&lt;string,string&gt;\nInformation on poisoned microblock if applicable (Field marked as ‘null’ suggests missing information or not applicable.)"
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_prod_br_contracts",
    "href": "content/lakehouse/bronze.html#tbl_prod_br_contracts",
    "title": "Bronze Layer",
    "section": "tbl_prod_br_contracts",
    "text": "tbl_prod_br_contracts\nThe table records details of smart contracts on the Stacks blockchain. It tracks the transaction hash (tx_hash), whether the contract is the main (canonical) version, the contract’s address, and the associated block number. Contract-specific data like Clarity language version (clarity_version), source code, and ABI (Application Binary Interface) are also stored. Additionally, flags indicate if the contract pertains to a STX20 token (is_stx20) or a non-fungible token (is_nft), offering insights into the contract’s purpose and functionality. This table is vital for developers and analysts interested in smart contract deployment and usage patterns on Stacks.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\ntx_hash\nstring\nTransaction hash of the contract. Try searching it in explorer.hiro.so\n\n\ncanonical\nboolean\nCanonical Blocks are the building blocks that contribute to the continuous, unbroken chain of blocks in the blockchain, each linking to its predecessor, forming the indelible ledger that blockchain is celebrated for. You expect this to be true.\n\n\naddress\nstring\nAddress of the contract.\n\n\nblock_number\nbigint\nIn which block this contract recorded in.\n\n\nclarity_version\nbigint\nClarity version of the contract.\n\n\nsource_code\nstring\nThe source code of the contract.\n\n\nabi\nstring\nThe ABI (Application Binary Interface) of the contract.\n\n\nis_stx20\nboolean\nIndicates whether the contract is a Stacks 2.0 token (STX20).\n\n\nis_nft\nboolean\nIndicates whether the contract is a non-fungible token (NFT)."
  },
  {
    "objectID": "content/ortegegpt/overview.html",
    "href": "content/ortegegpt/overview.html",
    "title": "OrtegeGPT : Revolutionizing Blockchain Data Analysis",
    "section": "",
    "text": "Introduction\nOrtegeGPT represents the forefront of innovation in blockchain analytics, embodying our commitment to harnessing the power of advanced artificial intelligence to transform data interaction. Currently under rigorous development, OrtegeGPT is poised to become a pivotal tool in the realm of blockchain data analysis, leveraging the capabilities of Large Language Models (LLMs) to offer an unparalleled user experience.\n\n\nVision\nOur vision for OrtegeGPT is to democratize access to complex blockchain datasets, enabling users to query, analyze, and gain insights from blockchain data through intuitive, natural language commands. By integrating cutting-edge AI with our deep expertise in blockchain technology, OrtegeGPT aims to simplify the complexities of blockchain data, making it accessible to analysts, developers, and enthusiasts alike.\n\n\nDevelopment Status\nOrtegeGPT is currently in heavy development, with our team dedicated to refining its capabilities, ensuring robustness, and tailoring the platform to meet the nuanced needs of blockchain analytics. Our approach combines rigorous testing with continuous feedback loops to create a tool that not only meets but exceeds the expectations of our users.\n\n\nFeatures to Anticipate\nWhile we are keeping some details under wraps until closer to release, users can look forward to the following features in OrtegeGPT:\n\nReal-Time Querying: Instantly query blockchain data across multiple platforms using natural language, making data analysis more accessible and efficient.\nPredictive Analytics: Leverage the predictive power of OrtegeGPT to forecast trends, identify patterns, and make data-driven decisions in the blockchain space.\nCustom Data Integration: Seamlessly integrate your proprietary data with OrtegeGPT, enriching your analysis and insights.\nMultiple Distribution Channels: Access OrtegeGPT through various channels, including API access, web interface, and integrated solutions within Ortege Studio, ensuring flexibility and ease of use.\n\n\n\nLooking Ahead\nAs we continue to develop OrtegeGPT, our focus remains on creating a tool that will not only revolutionize how we interact with blockchain data but also empower our users to unlock new insights and drive innovation. We are excited about the future of OrtegeGPT and look forward to sharing more details with you soon.\n\n\nStay Informed\nWe understand the anticipation surrounding OrtegeGPT and are committed to keeping our community informed. Updates on development progress, feature highlights, and eventual release details will be shared through our official channels. Stay tuned for an exciting journey ahead with OrtegeGPT."
  },
  {
    "objectID": "content/lakehouse/silver.html",
    "href": "content/lakehouse/silver.html",
    "title": "Silver Layer",
    "section": "",
    "text": "The Silver layer on Bitcoin will focus on exploding the inputs and outputs from the raw data as well as focus on all the L2’s, Rollups and Sidechains that are building on Bitcoin\n\n\n\n\nThe vw_prod_sl_outputs table is a structured representation of transaction outputs derived from the raw transactional data in the Bronze Layer of the Ortege Lakehouse. This table breaks down the transaction outputs, which are the results of Bitcoin transactions, into a format that is easier to analyze and query.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier of the transaction from which the output is derived.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes the transaction.\n\n\nblock_date\ndate\nThe date when the block containing the transaction was added to the blockchain.\n\n\nis_coinbase\nboolean\nIndicates whether the transaction output is from a coinbase transaction, signifying new bitcoins entering circulation as a miner’s reward.\n\n\naddress\nstring\nThe Bitcoin address that is the recipient of this output.\n\n\nindex\nbigint\nThe position of this output within the transaction; the first output is indexed as 0.\n\n\nrequired_signatures\nbigint\nThe number of signatures required to spend this output.\n\n\nscript_asm\nstring\nThe assembly notation of the script that dictates how this output can be spent.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the output script.\n\n\ntype\nstring\nThe type of script used in the transaction output (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins held in this output, represented in Satoshis (the smallest unit of bitcoin).\n\n\n\n\n\n\nThe vw_prod_sl_inputs table is an expanded view of the inputs from the raw transaction data within the Bronze Layer of the Ortege Lakehouse. This table breaks down individual transaction inputs, providing a granular look at the data that constitutes the transactional elements of the Bitcoin network.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier for the transaction containing this input.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes this transaction.\n\n\nblock_date\ndate\nThe date when the block was added to the blockchain.\n\n\nis_coinbase\nboolean\nA flag indicating whether the input is part of a coinbase transaction.\n\n\naddress\nstring\nThe Bitcoin address associated with this input.\n\n\nindex\nbigint\nThe position of this input within the transaction.\n\n\nrequired_signatures\nbigint\nThe number of required signatures for the transaction this input is part of.\n\n\nscript_asm\nstring\nThe assembly notation of the script used in the input.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the script used in the input.\n\n\nsequence\nbigint\nA sequence number associated with this input, used for replacement transactions.\n\n\nspent_output_index\nbigint\nThe index of the output that this input is spending from.\n\n\nspent_transaction_hash\nstring\nThe hash of the transaction from which this input spends.\n\n\ntype\nstring\nThe type of script used in this input (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins being input, typically matching the value of the spent output.\n\n\n\n\n\n\n\nThe tbl_prod_s1_wallet_transactions table is designed to store transactional data related to Bitcoin wallets. This data helps in monitoring and auditing the balance changes in each wallet associated with the system.\nThis table is utilized by Ortege to:\n\nTrack incoming and outgoing transactions for each wallet.\nCalculate the real-time balance of each wallet.\nAudit transactions for compliance and reporting purposes.\nDetect patterns and analyze wallet activity.\n\n\n\n\nIn the blockchain community, terminology such as “layer-2 solutions,” “sidechains,” and “rollups” can often lead to confusion due to overlapping features and capabilities. For the sake of clarity and convenience within our documentation, we will refer to these various technologies collectively as “L2 solutions.” This encompasses any scalability and extensibility solutions that are built on top of the Bitcoin network to enhance its performance and functionality.\nCurrently, our data platform focuses exclusively on the Stacks L2 solution, which brings smart contract capabilities and decentralized applications to Bitcoin. As an integral part of our data ecosystem, the Stacks-related tables and views provide users with detailed information about transactions and interactions specific to the Stacks blockchain. Our dedication to thorough data representation ensures users have comprehensive insights into this innovative L2, bolstering their understanding and analysis of its growing impact on the Bitcoin network.\n\n\nTracks and catalogues every transaction on the Bitcoin network that pertains to the Stacks Layer-2 solution. This is achieved by scanning for the distinctive “X2” magic code, which is a unique identifier embedded within the OP_RETURN opcode of Bitcoin transaction outputs. The presence of this magic code signals a transaction that is related to the Stacks ecosystem.\nEach transaction captured by our table is then systematically classified and decoded. This process transforms the raw data into a structured and user-friendly format, thereby allowing our users to effortlessly discern the nature and details of Stacks-related activities taking place on the Bitcoin blockchain.\nThe table serves as a powerful tool for users who are interested in monitoring the interactions between Bitcoin and the Stacks L2 solution.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nid\nRepresents the transaction hash.\n\n\nblock_height\nThe block height the transaction occurred in.\n\n\nblock_date\nThe date the transaction occurred.\n\n\naddress\nAddress of the sender.\n\n\nindex\nThe sequence number of the output in a transaction, starting at 0.\n\n\nscript_asm\nThe script of the transaction. An OP_RETURN followed by specific codes at index 0 indicates a non-spendable output embedding different types of data.\n\n\nvalue\nThe amount associated with the output, typically zero for OP_RETURN outputs as they are not intended for transfer.\n\n\ntransaction_type\n- vrf - VRF key registrations relating to OP_RETURN 58325e (X2^). - commit - Transactions relating to OP_RETURN 58325b (X2[): Block commit. Every output after the index 0, represents an amount that the Stacks miner commits in Satoshi’s to be paid to Stacks stackers. The final transaction is the unspent change to send back to the miner. - stack-stx - Transaction relating to OP_RETURN 583278 (X2x). - transfer-stx - Transactions relating OP_RETURN 583224 (X2$). - preSTX - Transactions relating to OP_RETURN 583270 (X2p).\n\n\ncommit\nA boolean reflecting true if the value is an amount of Satoshi’s committed to STX miners.\n\n\nburnt\nHighlights transactions that send BTC to the burn address 1111111111111111111114oLvT2, indicating that miners burn BTC.\n\n\n\n\n\n\n\n\n\n\nCaptures the rewards earned by Stacks (STX) token stackers, expressed in satoshis, which is the smallest unit of Bitcoin. It logs the unique identifier of the reward transaction (id), the block height at which the reward was recorded (block_height), the date of the block (block_date), the recipient address (address), and the amount of the reward (input_value).\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\n\n\n\n\nid\nInteger\nThe unique identifier for the stacking reward record.\n\n\nblock_height\nInteger\nThe height of the block in the Stacks blockchain.\n\n\nblock_date\nDate\nThe date when the block was mined.\n\n\naddress\nString\nThe Stacks wallet address that received the reward.\n\n\ninput_value\nInteger/Long\nThe reward amount earned for stacking, denominated in satoshis.\n\n\n\n\n\n\n\nBitcoin metaprotocols, such as Ordinals and BRC-20 tokens, represent innovative layers or protocols built on top of the Bitcoin blockchain that extend its functionality beyond simple financial transactions. These metaprotocols leverage the security and decentralization of Bitcoin while introducing new capabilities, such as the creation of smart contracts, tokens, and unique digital assets.\n\n\nOrdinals introduce the concept of “digital artifacts” to the Bitcoin ecosystem. Unlike traditional Bitcoin transactions that are purely financial, Ordinals allow for the inscription of arbitrary data directly onto individual satoshis (the smallest unit of Bitcoin). This innovation means that any piece of digital content, whether it be text, images, audio, or even small programs, can be inscribed onto the Bitcoin blockchain. Each inscribed satoshi becomes a unique, non-fungible token (NFT) due to its specific ordinal number, making it possible to own, trade, and collect digital artifacts in a way that’s secured by the Bitcoin network.\nKey Features:\n\nUniqueness: Each inscription is unique, tied to a specific satoshi.\nDecentralization: Utilizes the secure and decentralized nature of Bitcoin.\nVersatility: Allows for a wide range of digital content to be stored directly on the blockchain.\n\n\n\n\nBRC-20 tokens represent a standard for issuing fungible tokens on the Bitcoin blockchain, analogous to Ethereum’s ERC-20 standard. By utilizing Bitcoin sidechains or layers such as the Liquid Network or smart contract platforms built on top of Bitcoin, BRC-20 tokens enable the creation of custom tokens that can represent various assets, from traditional securities to new forms of digital assets. These tokens can be used for a multitude of purposes, including decentralized finance (DeFi), governance, and digital collectibles, all while benefiting from the security and liquidity of the Bitcoin network.\nKey Features:\n\nFungibility: Tokens conforming to the BRC-20 standard are interchangeable, with each token being identical to another in value and properties.\nSmart Contracts: Enables complex financial operations and agreements to be executed automatically.\nBitcoin Security: Leverages the robust security features of the Bitcoin blockchain."
  },
  {
    "objectID": "content/lakehouse/silver.html#base-tables",
    "href": "content/lakehouse/silver.html#base-tables",
    "title": "Silver Layer",
    "section": "",
    "text": "The vw_prod_sl_outputs table is a structured representation of transaction outputs derived from the raw transactional data in the Bronze Layer of the Ortege Lakehouse. This table breaks down the transaction outputs, which are the results of Bitcoin transactions, into a format that is easier to analyze and query.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier of the transaction from which the output is derived.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes the transaction.\n\n\nblock_date\ndate\nThe date when the block containing the transaction was added to the blockchain.\n\n\nis_coinbase\nboolean\nIndicates whether the transaction output is from a coinbase transaction, signifying new bitcoins entering circulation as a miner’s reward.\n\n\naddress\nstring\nThe Bitcoin address that is the recipient of this output.\n\n\nindex\nbigint\nThe position of this output within the transaction; the first output is indexed as 0.\n\n\nrequired_signatures\nbigint\nThe number of signatures required to spend this output.\n\n\nscript_asm\nstring\nThe assembly notation of the script that dictates how this output can be spent.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the output script.\n\n\ntype\nstring\nThe type of script used in the transaction output (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins held in this output, represented in Satoshis (the smallest unit of bitcoin).\n\n\n\n\n\n\nThe vw_prod_sl_inputs table is an expanded view of the inputs from the raw transaction data within the Bronze Layer of the Ortege Lakehouse. This table breaks down individual transaction inputs, providing a granular look at the data that constitutes the transactional elements of the Bitcoin network.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier for the transaction containing this input.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes this transaction.\n\n\nblock_date\ndate\nThe date when the block was added to the blockchain.\n\n\nis_coinbase\nboolean\nA flag indicating whether the input is part of a coinbase transaction.\n\n\naddress\nstring\nThe Bitcoin address associated with this input.\n\n\nindex\nbigint\nThe position of this input within the transaction.\n\n\nrequired_signatures\nbigint\nThe number of required signatures for the transaction this input is part of.\n\n\nscript_asm\nstring\nThe assembly notation of the script used in the input.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the script used in the input.\n\n\nsequence\nbigint\nA sequence number associated with this input, used for replacement transactions.\n\n\nspent_output_index\nbigint\nThe index of the output that this input is spending from.\n\n\nspent_transaction_hash\nstring\nThe hash of the transaction from which this input spends.\n\n\ntype\nstring\nThe type of script used in this input (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins being input, typically matching the value of the spent output."
  },
  {
    "objectID": "content/lakehouse/silver.html#wallet-transactions-tbl_prod_sl_wallet_transactions",
    "href": "content/lakehouse/silver.html#wallet-transactions-tbl_prod_sl_wallet_transactions",
    "title": "Silver Layer",
    "section": "",
    "text": "The tbl_prod_s1_wallet_transactions table is designed to store transactional data related to Bitcoin wallets. This data helps in monitoring and auditing the balance changes in each wallet associated with the system.\nThis table is utilized by Ortege to:\n\nTrack incoming and outgoing transactions for each wallet.\nCalculate the real-time balance of each wallet.\nAudit transactions for compliance and reporting purposes.\nDetect patterns and analyze wallet activity."
  },
  {
    "objectID": "content/lakehouse/silver.html#layer-2s",
    "href": "content/lakehouse/silver.html#layer-2s",
    "title": "Silver Layer",
    "section": "",
    "text": "In the blockchain community, terminology such as “layer-2 solutions,” “sidechains,” and “rollups” can often lead to confusion due to overlapping features and capabilities. For the sake of clarity and convenience within our documentation, we will refer to these various technologies collectively as “L2 solutions.” This encompasses any scalability and extensibility solutions that are built on top of the Bitcoin network to enhance its performance and functionality.\nCurrently, our data platform focuses exclusively on the Stacks L2 solution, which brings smart contract capabilities and decentralized applications to Bitcoin. As an integral part of our data ecosystem, the Stacks-related tables and views provide users with detailed information about transactions and interactions specific to the Stacks blockchain. Our dedication to thorough data representation ensures users have comprehensive insights into this innovative L2, bolstering their understanding and analysis of its growing impact on the Bitcoin network.\n\n\nTracks and catalogues every transaction on the Bitcoin network that pertains to the Stacks Layer-2 solution. This is achieved by scanning for the distinctive “X2” magic code, which is a unique identifier embedded within the OP_RETURN opcode of Bitcoin transaction outputs. The presence of this magic code signals a transaction that is related to the Stacks ecosystem.\nEach transaction captured by our table is then systematically classified and decoded. This process transforms the raw data into a structured and user-friendly format, thereby allowing our users to effortlessly discern the nature and details of Stacks-related activities taking place on the Bitcoin blockchain.\nThe table serves as a powerful tool for users who are interested in monitoring the interactions between Bitcoin and the Stacks L2 solution.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nid\nRepresents the transaction hash.\n\n\nblock_height\nThe block height the transaction occurred in.\n\n\nblock_date\nThe date the transaction occurred.\n\n\naddress\nAddress of the sender.\n\n\nindex\nThe sequence number of the output in a transaction, starting at 0.\n\n\nscript_asm\nThe script of the transaction. An OP_RETURN followed by specific codes at index 0 indicates a non-spendable output embedding different types of data.\n\n\nvalue\nThe amount associated with the output, typically zero for OP_RETURN outputs as they are not intended for transfer.\n\n\ntransaction_type\n- vrf - VRF key registrations relating to OP_RETURN 58325e (X2^). - commit - Transactions relating to OP_RETURN 58325b (X2[): Block commit. Every output after the index 0, represents an amount that the Stacks miner commits in Satoshi’s to be paid to Stacks stackers. The final transaction is the unspent change to send back to the miner. - stack-stx - Transaction relating to OP_RETURN 583278 (X2x). - transfer-stx - Transactions relating OP_RETURN 583224 (X2$). - preSTX - Transactions relating to OP_RETURN 583270 (X2p).\n\n\ncommit\nA boolean reflecting true if the value is an amount of Satoshi’s committed to STX miners.\n\n\nburnt\nHighlights transactions that send BTC to the burn address 1111111111111111111114oLvT2, indicating that miners burn BTC.\n\n\n\n\n\n\n\n\n\n\nCaptures the rewards earned by Stacks (STX) token stackers, expressed in satoshis, which is the smallest unit of Bitcoin. It logs the unique identifier of the reward transaction (id), the block height at which the reward was recorded (block_height), the date of the block (block_date), the recipient address (address), and the amount of the reward (input_value).\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\n\n\n\n\nid\nInteger\nThe unique identifier for the stacking reward record.\n\n\nblock_height\nInteger\nThe height of the block in the Stacks blockchain.\n\n\nblock_date\nDate\nThe date when the block was mined.\n\n\naddress\nString\nThe Stacks wallet address that received the reward.\n\n\ninput_value\nInteger/Long\nThe reward amount earned for stacking, denominated in satoshis."
  },
  {
    "objectID": "content/lakehouse/silver.html#metaprotocols",
    "href": "content/lakehouse/silver.html#metaprotocols",
    "title": "Silver Layer",
    "section": "",
    "text": "Bitcoin metaprotocols, such as Ordinals and BRC-20 tokens, represent innovative layers or protocols built on top of the Bitcoin blockchain that extend its functionality beyond simple financial transactions. These metaprotocols leverage the security and decentralization of Bitcoin while introducing new capabilities, such as the creation of smart contracts, tokens, and unique digital assets.\n\n\nOrdinals introduce the concept of “digital artifacts” to the Bitcoin ecosystem. Unlike traditional Bitcoin transactions that are purely financial, Ordinals allow for the inscription of arbitrary data directly onto individual satoshis (the smallest unit of Bitcoin). This innovation means that any piece of digital content, whether it be text, images, audio, or even small programs, can be inscribed onto the Bitcoin blockchain. Each inscribed satoshi becomes a unique, non-fungible token (NFT) due to its specific ordinal number, making it possible to own, trade, and collect digital artifacts in a way that’s secured by the Bitcoin network.\nKey Features:\n\nUniqueness: Each inscription is unique, tied to a specific satoshi.\nDecentralization: Utilizes the secure and decentralized nature of Bitcoin.\nVersatility: Allows for a wide range of digital content to be stored directly on the blockchain.\n\n\n\n\nBRC-20 tokens represent a standard for issuing fungible tokens on the Bitcoin blockchain, analogous to Ethereum’s ERC-20 standard. By utilizing Bitcoin sidechains or layers such as the Liquid Network or smart contract platforms built on top of Bitcoin, BRC-20 tokens enable the creation of custom tokens that can represent various assets, from traditional securities to new forms of digital assets. These tokens can be used for a multitude of purposes, including decentralized finance (DeFi), governance, and digital collectibles, all while benefiting from the security and liquidity of the Bitcoin network.\nKey Features:\n\nFungibility: Tokens conforming to the BRC-20 standard are interchangeable, with each token being identical to another in value and properties.\nSmart Contracts: Enables complex financial operations and agreements to be executed automatically.\nBitcoin Security: Leverages the robust security features of the Bitcoin blockchain."
  },
  {
    "objectID": "content/lakehouse/silver.html#proof-of-transfer",
    "href": "content/lakehouse/silver.html#proof-of-transfer",
    "title": "Silver Layer",
    "section": "Proof of Transfer",
    "text": "Proof of Transfer\nProof of Transfer, or PoX, is a novel consensus mechanism implemented by the Stacks blockchain, which is built on top of Bitcoin. The mechanism allows participants to lock up (or “stack”) their STX tokens temporarily to support the network’s security and consensus. In return, stackers receive rewards in Bitcoin. This process is governed by the smart contract SP000000000000000000002Q6VF78.pox-3\n\ntbl_prod_sl_pox3_stackers_solo\nTracks various Proof of Transfer (PoX) and Stacking operations on the Stacks blockchain. It includes solo stacking transactions. The transactions are filtered to only include successful ones pertaining to the pox-3 smart contract and are related to solo stacking operations such as stack-stx, stack-extend, and stack-increase.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nto_timestamp(block_timestamp)\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call\nAn object containing details of the contract call, including function name, contract ID, signature, and arguments.\n\n\ncontract_call.function_name\nThe name of the function called within the contract, indicating the type of stacking operation performed.\n\n\ncontract_call.contract_id\nThe ID of the contract that includes the function being called.\n\n\ncontract_call.function_signature\nThe signature of the function being called in the contract, representing the function’s interface.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\namount (conditional)\nThe amount of STX locked for stacking, normalized to STX units (converted from microSTX to STX with a decimal precision of 6), depending on the stacking operation.\n\n\nbtcAddress_to_receive_rewards\nThe Bitcoin address set to receive stacking rewards, extracted and decoded from the function arguments.\n\n\nlock_period (conditional)\nFor stack-stx operations, this represents the number of cycles the tokens are locked for stacking.\n\n\nstart_burn_height (conditional)\nFor stack-stx operations, this is the burnchain block height at which the stacking period starts.\n\n\nextend_cycle (conditional)\nFor stack-extend operations, this signifies the additional number of cycles for which an existing stack is extended.\n\n\n\n\n\nvw_prod_sl_pox3_stackers_delegates\nThis dataset focuses on delegated stacking operations, capturing the delegation of STX tokens for stacking purposes. It tracks the delegation transactions, detailing the delegators, delegatees, and the amounts involved.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nblock_timestamp\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call\nAn object containing details of the contract call, including function name, contract ID, signature, and arguments.\n\n\ncontract_call.function_name\nThe name of the function called within the contract, indicating the type of delegated stacking operation.\n\n\ncontract_call.contract_id\nThe ID of the contract that includes the function being called.\n\n\ncontract_call.function_signature\nThe signature of the function being called in the contract, representing the function’s interface.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\namount (conditional)\nThe amount of STX delegated for stacking, specified in the function arguments, depending on the operation type.\n\n\ndelegate_to (conditional)\nThe address to which the STX tokens are being delegated for stacking purposes.\n\n\n\n\n\nvw_prod_sl_pox3_agg\nThis dataset captures transactions pertaining to partial stacking commitments, allowing for an analysis of stacking operations that aggregate STX tokens from multiple sources. It provides insights into the delegation process, the amount of STX committed, and the intended reward cycles.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nblock_timestamp\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\ncontract_call\nAn object containing details of the contract call, including function name, contract ID, and signature.\n\n\ncontract_call.function_name\nThe specific function called within the contract, indicating the type of stacking operation.\n\n\ncontract_call.contract_id\nThe ID of the contract that includes the function being called.\n\n\ncontract_call.function_signature\nThe signature of the function being called in the contract.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\namount\nThe amount of microSTX committed to stacking, extracted from the contract log’s event data.\n\n\ndelegator\nThe address of the delegator, if applicable, extracted from the contract log’s event data.\n\n\npox_address\nThe Bitcoin address intended to receive the stacking rewards, extracted from the contract log’s event data.\n\n\nreward_cycle\nThe specific reward cycle(s) for which the stacking commitment is made, extracted from the contract log’s event data.\n\n\nstacker\nThe address of the stacker, extracted from the contract log’s event data."
  },
  {
    "objectID": "content/lakehouse/silver.html#dapps",
    "href": "content/lakehouse/silver.html#dapps",
    "title": "Silver Layer",
    "section": "dApps",
    "text": "dApps\nOur platform offers a specialized transformation of transaction data to cater to decentralized applications (dApps), particularly those in the DeFi sector. By processing raw transaction data, we extract and refine critical metrics such as trading volumes from decentralized exchanges (DEXs), collateral details from lending platforms, and token activities related to liquid staking.\nThis data transformation allows users to effortlessly access DeFi-specific insights, supporting informed decisions in trading, lending, and staking within the ecosystem. Our goal is to streamline complex datasets into actionable intelligence for dApp users and developers.\n\nGeneral\nThese foundational tables are pivotal in enriching our suite of dApp-specific tables with essential data. They act as core sources that inform and enhance the utility of subsequent datasets, providing a robust bedrock for a more insightful aggregation of dApp activities.\n\nvw_test_sl_sip10_tokens\nPlease note This view is still in test as we are still adding support for Symbol, Name and Circulating supply.\nThis view provides metadata about all tokens that comply with the SIP010 Fungible Token standard. This includes critical information such as the token address and its decimal precision, which is essential for accurately representing token quantities and facilitating token-related operations.\n\n\n\nColumn\nDescription\n\n\n\n\naddress\naddress of the SIP010 token\n\n\ndecimals\nthe amount of decimals of the token\n\n\nsymbol\nthe symbol of the token\n\n\nname\nthe name of the token\n\n\n\n\n\nvw_prod_sl_bns\nThe view retrieves successful transactions related to BNS (Bitcoin Name Service) domain names, such as preorders, registrations, transfers, renewals, and other updates. It aims to offer insights into the usage and activities around the Stacks domain name services.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\ntx_date\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nfunction_name\nThe specific function called within the bns contract, indicating the type of domain name operation.\n\n\namount\nConditionally shows the asset amount for name-preorder and the renewal fee for name-renewal.\n\n\nname\nFor name-register, name-transfer, and name-renewal, this is the domain name involved.\n\n\nnew_owner\nFor name-transfer operations, this indicates the new owner’s address of the domain name.\n\n\n\n\n\n\nDEXs\nWe aggregate and refine swap volume data from leading decentralized exchanges, specifically ALEX and Bitflow. By streamlining the collection of trade data through direct blockchain interactions, the Silver Layer presents a unified and enriched view of the cryptocurrency market’s liquidity and trading activity. This integration allows users to access detailed records of swap events, providing insights into trading pair performances, volume fluctuations, and liquidity provisions. Our users leverage this high-fidelity data for in-depth market analysis, deriving strategic intelligence essential for traders and financial analysts who demand the most current and comprehensive market snapshots.\n\nvw_prod_sl_dapp_alex_swap\nThe vw_dev_sl_dapp_alex_swap view provides a comprehensive look into the smart contract activity of the ALEX dApp, showcasing token swaps. It includes transaction timestamps, block numbers, contract identifiers, and detailed function arguments, facilitating an in-depth analysis of dApp interactions on the Stacks blockchain.\nPlease note This layer includes the volume in the native currency of each token and doesn’t include pricing details. If you’d like it converted by the token’s decimals and to have the realtime pricing information, that is available on the Gold layer.\n\n\n\nColumn\nDescription\n\n\n\n\nblock_timestamp\nTimestamp of the block (datetime)\n\n\nblock_number\nNumber of the block (bigint)\n\n\nhash\nUnique identifier for a transaction (varchar)\n\n\nsender_address\nAddress of the sender (varchar)\n\n\ncontract_id\nIdentifier of the contract (varchar)\n\n\nfunction_name\nName of the function called (varchar)\n\n\nfunction_args\nArguments passed to the function (struct)\n\n\ntokenA\nIdentifier for token A (varchar)\n\n\ntokenB\nIdentifier for token B (varchar)\n\n\ntokenAOut\nAmount of token A output (bigint)\n\n\ntokenBIn\nAmount of token B input (varchar)\n\n\n\nPlease note This chart needs some improvements which will be found in the Gold Layer. Take note the volume has all been divided by 8 decimals places. This is hardcoded and some of those tokens may not have 8 decimal places so to get accurate figures you’d need to get the decimals from vw_dev_sl_sip10_tokens\n\n\nvw_prod_sl_dapp_bitflow\nThis dataset focuses on successful liquidity and swap event transactions. It covers add-liquidity, swap-x-for-y, swap-y-for-x, and withdraw-liquidity operations. Each record provides a transaction hash, the sender’s address, the block number, the date and time of the transaction (tx_date), an array of events (events), the function name indicating the operation (contract_call.function_name), and the function arguments (contract_call.function_args). The dataset also includes calculated fields for the amounts of token X and token Y involved in each transaction, with positive values typically representing token acquisitions and negative values indicating disbursements or deductions.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\ntx_date\nThe timestamp of the block, formatted as a human-readable date and time.\n\n\nevents\nAn array of events that were triggered by the transaction.\n\n\ncontract_call.function_name\nThe name of the function called, indicating the type of operation (e.g., adding liquidity, swapping, etc.).\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\ntokenY\nRepresents the identifier or amount of token Y involved, extracted from the first function argument.\n\n\ntokenX_amount (conditional)\nThe calculated amount of token X involved in the operation, depending on the function name.\n\n\ntokenY_amount (conditional)\nThe calculated amount of token Y involved in the operation, depending on the function name.\n\n\n\n\n\n\nLiquid Staking (Stacking) Tokens (LST)\nLiquid staking tokens are innovative financial instruments in the DeFi ecosystem, allowing participants to stake cryptocurrencies and receive tokenized derivatives in return. These derivatives represent the staked assets, offering liquidity and enabling holders to engage in additional DeFi activities without unbonding their original stakes.\nKey Features: * Liquidity: Users can access their staked value for use in other DeFi protocols. * Yield Optimization: Allows earning on staked assets while engaging in other yield-generating activities. * Flexibility and Security: Facilitates diverse investment strategies and mitigates certain risks associated with direct staking.\nUse Cases: * Used in yield farming, as collateral for loans, and for trading to speculate or hedge positions. Risks: * Involves smart contract risks, potential liquidity issues, and regulatory uncertainties.\nLiquid staking tokens enhance capital efficiency in the blockchain realm, making staked assets work harder for the holder. They represent a key development towards making DeFi more accessible and versatile, though participants should be mindful of inherent risks.\n\nvw_dev_sl_dapp_stacking_dao\nThis dataset includes successful transaction records for staking-related operations (deposit, withdraw, init-withdraw, add-rewards) on the Stacks blockchain. Each record in the dataset includes the transaction hash, sender address, block number, and a human-readable transaction date. It also details the events triggered, the specific function called within the staking contract, and the arguments provided to the function call. The dataset captures the amount involved in the transactions and, for certain operations, the NFT number and cycle number related to the transaction.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\ntx_date\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call.function_name\nThe specific function called within the staking contract.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\ntx_result\nThe result of the transaction execution, containing details about the success or failure of the operation.\n\n\ntx_status\nThe status of the transaction, indicating whether it was successful.\n\n\namount (conditional)\nThe token amount involved in the deposit, init-withdraw, withdraw, or add-rewards operations, based on the function name.\n\n\nnft_number (conditional)\nThe NFT number associated with the init-withdraw and withdraw operations.\n\n\ncycle (conditional)\nFor add-rewards operations, this field indicates the cycle number for which rewards are being added.\n\n\n\n\n\n\nBridges\n\nvw_dev_sl_bridge_abtc\nThis dataset captures successful transaction events for ABTC. The dataset includes transactions for transfer, mint-fixed, and burn-fixed operations. Each record details the transaction hash, sender address, block number, timestamp of the block, the events array, the name of the function called in the contract (function_name), and the arguments passed to the function (function_args). Additionally, it extracts the amount for transfer, burn-fixed, and mint-fixed functions, and the recipient for transfer function.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nto_timestamp(block_timestamp)\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call.function_name\nThe specific function called within the contract, indicating the type of token operation performed.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\namount (conditional)\nThe token amount involved in the transfer, mint-fixed, or burn-fixed operations, extracted from the function arguments.\n\n\nrecipient (conditional)\nFor transfer operations, the address of the recipient of the tokens, extracted from the function arguments.\n\n\n\n\n\n\nMetaTokens\nMetatokens, including standards like BRC20, STX20, and Ordinals, are protocols for representing a variety of digital assets on blockchain networks. These standards are pivotal for creating tokens that are fungible or non-fungible, depending on the use case, and ensure interoperability across different applications and services within the blockchain ecosystem.\nThese metatokens facilitate the digital representation of a wide array of assets—ranging from currencies, utility tokens, and governance tokens, to unique assets like collectibles or digital art. They define a common set of rules that the tokens must adhere to, which typically includes functions for transferring tokens, inquiring balances of addresses, and granting permission for one address to transfer tokens on behalf of another. This common framework allows for the consistent development of wallets, exchanges, decentralized applications, and even complex financial instruments, fostering an environment where digital assets can be easily created, managed, and exchanged.\n\nvw_dev_sl_stx20\nThe vw_dev_sl_stx20 view captures transactions involving STX20 tokens, including minting, deploying, and transferring operations. It includes transaction identifiers, participant addresses, transaction dates, and operation types, among other details. This view is vital for tracking STX20 token movements and understanding their supply dynamics.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nhash\nUnique identifier for a transaction (varchar)\n\n\nsender_address\nAddress of the sender (varchar)\n\n\nfee_rate\nTransaction fee rate (varchar)\n\n\nrecipient_address\nAddress of the recipient (varchar)\n\n\nstx_paid\nAmount of STX paid (varchar)\n\n\ndate\nDate of the transaction (date)\n\n\nmemo_text\nText of the memo field (varchar)\n\n\nSTX_operation\nType of operation (mint, deploy, or transfer) (enum)\n\n\nstx20_ticker\nTicker symbol for the token (varchar)\n\n\namount\nAmount of tokens (varchar)\n\n\ntotal_supply\nTotal supply of tokens (varchar)\n\n\nlimit_per_mint\nLimit per mint operation (varchar)"
  },
  {
    "objectID": "content/lakehouse/silver.html#envelopes",
    "href": "content/lakehouse/silver.html#envelopes",
    "title": "Silver Layer",
    "section": "Envelopes",
    "text": "Envelopes"
  },
  {
    "objectID": "content/lakehouse/medallian.html",
    "href": "content/lakehouse/medallian.html",
    "title": "Medallian Architecture",
    "section": "",
    "text": "Ortege Lakehouse is designed around the innovative Medallion Architecture, a tiered data management strategy that systematically categorizes data into three distinct layers: Bronze, Silver, and Gold. This architecture facilitates a streamlined progression of data from raw ingestion to refined analytics, ensuring users across various roles and functions can access the most relevant and optimized data for their needs. The structured approach underpinning the Medallion Architecture reflects our commitment to delivering clarity, efficiency, and value in data analysis."
  },
  {
    "objectID": "content/lakehouse/medallian.html#bronze-layer",
    "href": "content/lakehouse/medallian.html#bronze-layer",
    "title": "Medallian Architecture",
    "section": "Bronze Layer:",
    "text": "Bronze Layer:\nThe foundation of the Medallion Architecture, the Bronze Layer, houses raw data directly ingested from diverse sources. This layer prioritizes the authenticity and completeness of data, serving as the primary repository for unprocessed information. Views that reference tables within the Bronze Layer remain in this tier, preserving the raw state of data until specific transformations are applied."
  },
  {
    "objectID": "content/lakehouse/medallian.html#silver-layer",
    "href": "content/lakehouse/medallian.html#silver-layer",
    "title": "Medallian Architecture",
    "section": "Silver Layer:",
    "text": "Silver Layer:\nData transitions to the Silver Layer following initial processing, which includes cleansing, normalization, and structuring. This layer enhances the usability of data, making it more accessible and meaningful for analysis. The Silver Layer acts as a bridge between raw data and advanced analytics, providing a balanced dataset that is both rich in detail and optimized for exploration."
  },
  {
    "objectID": "content/lakehouse/medallian.html#gold-layer",
    "href": "content/lakehouse/medallian.html#gold-layer",
    "title": "Medallian Architecture",
    "section": "Gold Layer:",
    "text": "Gold Layer:\nRepresenting the pinnacle of data refinement, the Gold Layer contains datasets that are highly curated and performance-optimized for specific analytical use cases. Data in this layer is tailored to deliver actionable insights, supporting decision-making processes with precision and speed. The Gold Layer embodies the ultimate goal of data transformation within Ortege Lakehouse, offering users targeted, ready-to-use datasets for in-depth analysis."
  },
  {
    "objectID": "content/lakehouse/gold.html",
    "href": "content/lakehouse/gold.html",
    "title": "Gold Layer",
    "section": "",
    "text": "The primary purpose of the Gold Layer is to refine, consolidate, and curate datasets that are critical for the development and enhancement of Language Learning Models (LLM), Machine Learning (ML) models, and aggregate data models. In the rapidly evolving landscape of cryptocurrencies, the ability to derive meaningful insights from vast amounts of data is paramount. The Gold Layer is dedicated to providing a structured and enriched dataset that enables accurate modeling, forecasting, and analysis.\nAs cryptocurrencies continue to grow in both complexity and volume, the need for an advanced, reliable data foundation becomes increasingly important. The Gold Layer addresses this need by offering a comprehensive dataset solution that encompasses everything from individual decentralized exchange (DEX) volumes to complex, aggregated data models. This layer is not just about data storage; it’s about transforming raw data into a goldmine of insights that power our language and machine learning models, as well as our innovative aggregate data models.\nOur commitment to excellence is reflected in the meticulous design and functionality of the Gold Layer. By leveraging state-of-the-art technologies and methodologies, we ensure that the data within this layer is of the highest quality, accuracy, and reliability. Whether you’re developing advanced ML models or seeking to consolidate numerous DEX volumes into a cohesive aggregation layer, the Gold Layer provides the essential datasets needed to drive your analytics forward.\nIn the following sections, we will delve deeper into the architecture and design of the Gold Layer, the types of datasets available, their use cases, and how you can access and integrate these datasets into your analytical tools and models. Join us as we explore the foundational layer that is setting new standards in cryptocurrency analytics."
  },
  {
    "objectID": "content/lakehouse/gold.html#proof-of-transfer",
    "href": "content/lakehouse/gold.html#proof-of-transfer",
    "title": "Gold Layer",
    "section": "Proof of Transfer",
    "text": "Proof of Transfer\n\ntbl_prod_gld_pox3_stackers_all\nThis table joins all Stacking STX transactions with all BTC reward transactions.\nIt unions the following data sources:\n\nStack STX via the Bitcoin chain\nBTC rewards for Stacking STX\nStack STX via the Stacks chain\nDelegate STX via the Stacks chain\n\n\nStack STX via the Bitcoin chain\nIt is possible to initiate stacking from the Bitcoin chain. We identify these transactions via the X2 magic code for Bitcoin transactions.\n\n\nBTC rewards for Stacking STX\nTo get the rewards we identify every transaction as it flows to the Stacking address. This follows the following path: Stacks Miner commits BTC in a transaction -&gt; Track the Input of the Committed BTC -&gt; Report on the output from the Input.\nFor example if a Stacks miner commits 3 values of 1,2 and 5 BTC in one Bitcoin transaction, we will report all three commits separately as rewards. This is to ensure that we have a full audit trail of the supply chain of the BTC from Miners to Stackers.\n\n\nStack STX via the Stacks chain\nDue to the way the POX3 contract was designed, getting the cycles for the stack-extend and stack-increase involves a bit of complexity. Stack extend extends an existing stack-stx transaction, so we get the cycle from the stack-extend transactions by looking at the original stack-stx transaction and it’s lock cycle and then adding 1. For example if Stacker A stacked 120k STX for 5 cycles starting on cycle 60, then that will be reported as 120k STX cycle 60 and lock period of 5. But if that is then extended by 2 cycles, there will be another record returned with 120k STX for cycle 66 and a lock period of 2.\nFor stack-increase we use the block height of the transaction to determine the cycle the STX will be locked in.\n\n\nDelegate STX via the Stacks chain\nThe delegate-stx and delegate-stack-stx functions in PoX3 both deal with the delegation of STX tokens for Stacking, but they serve different purposes and operate in distinct contexts.\ndelegate-stx The delegate-stx function is used to delegate the authority to lock STX for Stacking to another address (a delegate). When a user calls delegate-stx, they specify how much STX they want to delegate, the delegate’s address, an optional burn height at which the delegation expires, and an optional PoX address where the rewards should be sent. This function does not lock STX tokens itself but grants the specified delegate the permission to lock the user’s STX tokens on their behalf.\nKey points about delegate-stx:\n\nIt sets up a delegation relationship allowing the delegate to later initiate Stacking on behalf of the delegator.\nIt can specify a PoX address for rewards and an expiration for the delegation. It is useful for users who wish to participate in Stacking indirectly through a service or pool.\n\ndelegate-stack-stx The delegate-stack-stx function is utilized by the delegate (an address given permission via delegate-stx) to actually lock the delegated STX tokens for Stacking. This function is called after delegate-stx and is the step where the STX tokens are actually committed to Stacking. When calling delegate-stack-stx, the delegate specifies the amount of STX to lock, the PoX address for rewards, the start burn height, and the lock period (number of reward cycles).\nKey points about delegate-stack-stx:\n\nIt is used by a delegate to lock STX tokens for Stacking on behalf of a delegator.\nIt requires prior setup via delegate-stx.\nIt initiates the actual Stacking process using the delegated STX tokens.\nIn summary, delegate-stx is about setting up the delegation relationship and granting permission, while delegate-stack-stx is about the delegate taking action to lock the delegated STX for Stacking. The former is preparatory, and the latter is the execution step in the delegation process for participating in Stacking\n\nPlease note If you want a true figure for Stacking, you’ll need to exclude delegate-stx as they are committed but not finalized. We include them in this table here for completeness and an audit trail for users who are looking to debug their delegations.\n\n\n\ntbl_prod_gld_pox3_stackers_solo_cycles\nEach record in the table represents a unique stacking event by an individual stacker. The table records encompass a broad range of information, from basic transaction details to specific stacking parameters, such as the lock period and eligible cycles for rewards.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nUnique hash of the stacking transaction.\n\n\nsender_address\nCryptocurrency address of the stacker.\n\n\nblock_number\nBlock number where the transaction was recorded.\n\n\ntx_date\nDate and time when the transaction was processed.\n\n\nbtc_block_height\nBitcoin block height at the time of the transaction.\n\n\namount\nAmount of cryptocurrency stacked.\n\n\nbtc_reward_address_decoded\nDecoded Bitcoin address for reward eligibility.\n\n\nfunction_name\nName of the smart contract function invoked.\n\n\nlock_period\nNumber of cycles the funds are locked for stacking.\n\n\nstart_burn_height\nStarting burn block height for stacking.\n\n\nextend_cycle\nIndicator for extending the lock period.\n\n\nstart_cycle\nFirst eligible cycle for rewards.\n\n\nend_cycle\nLast eligible cycle for rewards."
  },
  {
    "objectID": "content/studio/datasets.html",
    "href": "content/studio/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets in Ortege Studio\nDatasets form the backbone of any analysis in Ortege Studio, serving as the structured collection of data upon which charts, dashboards, and insights are built. This section of the documentation will guide you through managing datasets, including how to import, prepare, and optimize them for analysis within Ortege Studio.\n\n\nImporting Datasets\n\nData Sources: Ortege Studio allows you to connect to a wide range of data sources, including relational databases, cloud storage solutions, and real-time data streams. Identify the source of your data to begin the import process.\nImport Process: Use the data import wizard in Ortege Studio to guide you through the steps of connecting to your data source and importing your dataset. This process includes specifying connection details, selecting the data to import, and defining import settings.\nValidation and Preview: Once your data is imported, validate and preview the dataset to ensure accuracy and completeness. This step is crucial for catching any issues before moving on to analysis.\n\n\n\nPreparing Datasets\n\nCleaning Data: Ortege Studio provides tools for cleaning and preprocessing your data, such as removing duplicates, handling missing values, and correcting data types. Clean data is essential for accurate analysis.\nDefining Metrics and Dimensions: Identify and define the key metrics and dimensions within your dataset. This categorization is critical for creating meaningful charts and dashboards later on.\nData Transformation: Apply transformations to your dataset to prepare it for analysis. This can include aggregating data, creating calculated fields, and applying filters.\n\n\n\nOptimizing Datasets\n\nPerformance Tuning: Large datasets can impact the performance of your visualizations. Use Ortege Studio’s optimization features, such as indexing and materialized views, to improve query performance and speed up load times.\nData Modeling: For complex analyses, consider building a data model within Ortege Studio. A well-structured data model can simplify analysis and improve performance by efficiently organizing data relationships.\nData Caching: Leverage Ortege Studio’s data caching capabilities to store frequently accessed data in memory, reducing the need for repeated queries to the data source and accelerating data retrieval.\n\n\n\nBest Practices for Dataset Management\n\nRegular Updates: Keep your datasets up to date with regular refreshes. This ensures your analyses are based on the most current data available.\nSecurity and Privacy: Implement data security and privacy measures to protect sensitive information. This includes managing access controls and anonymizing personal data where necessary.\nDocumentation: Document your datasets, including sources, transformations applied, and any known limitations or issues. Good documentation supports effective collaboration and analysis.\n\n\n\nAdvanced Dataset Features\n\nVersion Control: Maintain versions of your datasets to track changes over time. This is especially useful for datasets that undergo frequent updates or changes.\nCollaborative Editing: Ortege Studio supports collaborative dataset management, allowing multiple users to work on a dataset simultaneously. This feature facilitates teamwork and ensures consistency across analyses.\nIntegration with Machine Learning: Prepare and optimize your datasets for machine learning models. Ortege Studio’s integration with predictive analytics tools enables you to enrich your analyses with advanced data modeling techniques.\n\nDatasets are a critical component of the analytical process in Ortege Studio, enabling users to derive actionable insights from their data. By effectively managing your datasets—from import and preparation to optimization—you lay the groundwork for powerful data analysis and visualization within your organization."
  },
  {
    "objectID": "content/studio/sqllab.html",
    "href": "content/studio/sqllab.html",
    "title": "SQL Lab",
    "section": "",
    "text": "SQL Lab in Ortege Studio\nSQL Lab is a powerful feature within Ortege Studio that caters to data analysts and engineers who prefer to interact with their data through SQL queries. This integrated development environment (IDE) is designed for crafting, testing, and visualizing SQL queries in real-time, providing a direct and flexible approach to data exploration. This section of the documentation will introduce SQL Lab, detailing its key functionalities and offering guidance on how to effectively utilize this tool.\n\n\nKey Features of SQL Lab\n\nInteractive Query Editor: SQL Lab’s query editor supports syntax highlighting, auto-completion, and error detection, making it easier to write and debug SQL queries.\nQuery History: Keep track of all your queries with SQL Lab’s query history feature. This allows you to revisit, analyze, and repurpose previous queries, enhancing productivity and collaboration.\nData Visualization: After executing a query, SQL Lab enables you to visualize the results directly within the interface. This immediate feedback loop is invaluable for quick iterations and refinements of your analysis.\n\n\n\nGetting Started with SQL Lab\n\nAccess SQL Lab: Navigate to the SQL Lab interface from the main menu of Ortege Studio. Here, you’ll find the query editor, saved queries, and query history.\nConnect to a Database: Before you can start querying, you must connect SQL Lab to a database. Select your database from the dropdown menu of available connections, which are configured in the Database Connections section of Ortege Studio.\nWrite and Execute Your Query: In the query editor, write your SQL query. Execute the query by clicking the “Run Query” button. SQL Lab will process your query and display the results in the lower panel.\n\n\n\nAdvanced Querying in SQL Lab\n\nSaved Queries: SQL Lab allows you to save your queries for future use. This is particularly useful for complex queries that you run regularly or for sharing queries with team members.\nVisualization Options: Explore different visualization options for your query results. You can choose from various chart types and configurations to best represent your data insights.\nExport Results: SQL Lab provides options to export your query results for further analysis or reporting. Supported formats include CSV and Excel, among others.\n\n\n\nBest Practices for Using SQL Lab\n\nOptimize Your Queries: To ensure efficient data retrieval and processing, optimize your SQL queries. This includes selecting only necessary columns, using appropriate WHERE clauses to filter data, and leveraging indexes.\nUse Comments: Document your queries with comments to explain the purpose, logic, and any specific considerations. This practice improves readability and facilitates collaboration.\nUnderstand Your Data: Familiarize yourself with the structure and characteristics of your datasets. Understanding your data will help you craft more effective and accurate queries.\n\n\n\nTroubleshooting Common Issues\n\nPerformance Issues: For queries that take a long time to execute, consider optimizing your query or checking the database performance. Additionally, SQL Lab settings may allow for query time limits and concurrency settings to be adjusted.\nConnectivity Problems: If you encounter issues connecting to a database, verify your connection settings and ensure that the database server is accessible from Ortege Studio.\n\nSQL Lab in Ortege Studio offers a sophisticated environment for direct data querying and analysis, bridging the gap between traditional SQL development and modern data visualization. By leveraging SQL Lab, users can harness the full potential of their data, gaining deeper insights and driving more informed decisions across their organization."
  },
  {
    "objectID": "content/studio/charts.html",
    "href": "content/studio/charts.html",
    "title": "Charts",
    "section": "",
    "text": "Charts in Ortege Studio\nCharts are at the heart of data visualization in Ortege Studio, offering a dynamic way to represent data and uncover insights through a wide range of visual formats. This section of the documentation will walk you through the process of creating, customizing, and managing charts, ensuring you can fully leverage Ortege Studio’s powerful visualization capabilities.\n\n\nCreating a Chart\n\nSelect Dataset: Begin by choosing the dataset from which you want to create your chart. Ortege Studio supports a diverse array of datasets, ensuring you can visualize data from various sources.\nChoose Chart Type: Select the type of chart that best fits your data visualization needs. Ortege Studio offers an extensive selection, from traditional bar and line charts to more complex scatter plots and heatmaps.\nConfigure Chart: Customize your chart by specifying dimensions and metrics, applying filters, and setting aggregation functions. This step is crucial in defining how your data will be displayed.\n\n\n\nCustomizing Charts\n\nStyling Options: Use the chart’s styling options to enhance its appearance. You can adjust colors, fonts, axis labels, and legends to make your chart more informative and visually appealing.\nInteractive Features: Enhance user interaction by enabling tooltips, clickable elements, and drill-down capabilities. These features make your charts more engaging and informative.\nResponsive Design: Ensure your charts are responsive and look great on all devices. Ortege Studio allows you to preview and adjust chart layouts for different screen sizes.\n\n\n\nManaging Charts\n\nSaving and Organizing: Once you are satisfied with your chart, save it to your dashboard or a specific folder. Proper organization helps in easily accessing and managing your charts.\nSharing and Collaboration: Share your charts with team members for collaborative analysis or embed them in presentations and reports. Ortege Studio provides various sharing options, including direct links and embedding codes.\nVersion Control: Keep track of changes to your charts with version control. This feature allows you to revert to previous versions if needed and understand how your chart evolved over time.\n\n\n\nBest Practices for Chart Design\n\nSimplicity is Key: Aim for clarity and simplicity in your charts. Avoid unnecessary decorations that can distract from the data.\nChoose the Right Chart Type: Match the chart type with the nature of your data and the story you want to tell. Each chart type serves a specific purpose.\nUse Color Wisely: Use color to enhance comprehension, not just for aesthetics. Differentiate data points clearly and use consistent color schemes across similar charts.\n\n\n\nAdvanced Chart Features\n\nCustom SQL and Expressions: For advanced users, Ortege Studio supports the use of custom SQL queries and expressions to create more complex data visualizations.\nIntegration with Predictive Analytics: Leverage Ortege Studio’s integration capabilities to incorporate predictive analytics into your charts, providing forward-looking insights based on historical data trends.\nAPI Access: Automate chart creation and updates through Ortege Studio’s API, enabling dynamic and real-time data visualizations in your charts.\n\nCharts are a fundamental aspect of data visualization in Ortege Studio, enabling users to tell compelling stories with their data. By understanding how to effectively create, customize, and manage your charts, you can unlock the full potential of your data, providing valuable insights to drive decision-making within your organization."
  },
  {
    "objectID": "studio/datasets.html",
    "href": "studio/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets\nDatasets in Ortege Studio are foundational elements that represent the structured data you work with. They are usually tables or views from connected databases. Understanding the Datasets feature is key to effectively utilizing Ortege Studio for your data analysis and visualization needs.\n\nAccessing Datasets\nScreenshot needed: Accessing Datasets from the Ortege Studio menu\n\nNavigation: You can access the Datasets feature from the main Ortege Studio menu, typically found under the “Data” section.\nDatasets Page: This page lists all available datasets, showing their names, database connections, and other relevant details.\n\n\n\nDataset Overview\nScreenshot needed: Overview of a Dataset\n\nMetadata: Each dataset listing provides metadata, such as the database it belongs to, the schema, and when it was last updated.\nActions: From here, you can perform actions like editing dataset properties, deleting datasets, or exploring the dataset through visualizations.\n\n\n\nEditing a Dataset\nScreenshot needed: Dataset editing interface\n\nEditing Interface: By selecting a dataset, you enter the editing interface. This area allows you to modify various aspects of the dataset.\nChanging Metadata: You can change the dataset’s descriptive information, like its verbose name, description, and more.\n\n\n\nColumns and Metrics\nScreenshot needed: Columns and Metrics section of a dataset\n\nColumns Tab: This tab shows all the columns of your dataset, including their type and source. You can edit column details, like renaming them, changing data types, or adding calculated columns.\nMetrics Tab: Here, you can define and manage custom metrics that are specific to the dataset, like aggregations or formulas.\n\n\n\nDataset Usage\nScreenshot needed: Visualization using a selected dataset\n\nVisualization: Datasets can be directly used to create various visualizations and dashboards.\nSQL Lab Integration: Datasets also integrate seamlessly with SQL Lab, allowing for advanced querying and exploration.\n\n\n\nSecurity and Permissions\nScreenshot needed: Dataset’s security settings\n\nAccess Control: Ortege Studio enables fine-grained access control on datasets, allowing you to control who can view or edit them.\nRow-Level Security: You can also set up row-level security to ensure users only see data they are permitted to view.\n\n\n\nConclusion\nThe Datasets feature in Ortege Studio is a versatile and crucial component of the platform. It allows for the organized management and utilization of your data within Ortege Studio, facilitating everything from simple explorations to complex visualizations and analyses."
  },
  {
    "objectID": "studio/sqllab.html",
    "href": "studio/sqllab.html",
    "title": "SQL Lab",
    "section": "",
    "text": "SQL Lab\nSQL Lab is a feature-rich SQL editor within Ortege Studio designed for crafting and running complex queries. The interface is user-friendly and supports a variety of functions to enhance your data exploration experience. Let’s take a detailed tour of its interface.\n\nSQL Editor\nScreenshot needed: SQL Editor interface\nThe SQL Editor is the heart of SQL Lab. It’s where you write, edit, and run your SQL queries.\n\nQuery Tab: Located at the top, each tab represents a separate SQL query, allowing you to work on multiple queries simultaneously.\nSQL Editor Pane: This is the large text area where you write your SQL code. It features syntax highlighting and auto-completion to assist with writing queries efficiently.\nRun Button: Click this button to execute the query written in the active tab.\n\n\n\nResults Pane\nScreenshot needed: Results pane showing query results\nBelow the SQL Editor, you’ll find the Results Pane.\n\nData View: Once a query is run, the results are displayed here in a tabular format.\nRow Limit: The interface allows you to specify the number of rows to display, which is useful for large result sets.\nCSV Export: You can export the result of your query directly to a CSV file from this pane.\n\n\n\nData Preview\nScreenshot needed: Data preview of a selected table\n\nData Source Selector: To the left of the SQL Editor, you’ll find the Data Source Selector. Here, you can choose the database and schema you wish to query.\nTable Selector and Preview: Once a table is selected, a preview of the table’s data is displayed, along with column names and data types. This is particularly helpful for understanding the structure of your data before writing a query.\n\n\n\nQuery History\nScreenshot needed: Query history section\n\nAccessing History: The Query History section, accessible from the bottom left of the interface, keeps a log of all executed queries.\nDetails and Actions: Each entry in the history shows the query, the time of execution, and its status. You can also rerun or copy queries from this history.\n\n\n\nAdditional Features\n\nSave Query: You can save queries for future use. This option is located next to the Run button.\nSchema Browser: The Schema Browser, adjacent to the Data Source Selector, lists all tables in the selected schema and provides information about table columns, indexes, and partitions.\n\n\n\nConclusion\nThe SQL Lab interface in Ortege Studio is designed to be intuitive yet powerful, catering to both beginners and experienced SQL users. The seamless integration of query writing, data preview, and result analysis within a single interface makes SQL Lab an efficient tool for data exploration and analysis."
  }
]