[
  {
    "objectID": "studio/dashboards.html",
    "href": "studio/dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "Dashboards\nDashboards in Ortege Studio are powerful tools for combining various charts and visualizations into a single, interactive, and dynamic view. They provide a holistic view of your data, allowing for efficient monitoring and insightful decision-making.\n\nAccessing Dashboards\nScreenshot needed: Accessing Dashboards from the Ortege Studio menu\n\nNavigation: You can find Dashboards in the main Ortege Studio menu, often under a section labeled “Dashboards.”\nDashboard Page: This page displays all the available dashboards, with details like the dashboard name, owner, and last modified date.\n\n\n\nCreating a New Dashboard\nScreenshot needed: Creating a new dashboard interface\n\nStarting a New Dashboard: To create a new dashboard, click on the “+” or “New” button.\nNaming the Dashboard: Assign a name and optionally a description to your new dashboard for easy identification.\n\n\n\nAdding Charts to Dashboard\nScreenshot needed: Adding charts to a dashboard\n\nChart Selection: In the dashboard edit mode, you can add charts that you’ve previously created in the Charts section.\nPositioning and Resizing: Drag and drop the charts into your desired position on the dashboard. You can also resize them for optimal layout and visibility.\n\n\n\nCustomizing the Dashboard\nScreenshot needed: Customizing the dashboard layout and settings\n\nLayout Customization: You can customize the layout of the dashboard, organizing charts and components in a way that best tells your data story.\nFilter Options: Add interactive filters to the dashboard that allow users to dynamically change the data displayed across multiple charts.\n\n\n\nInteracting with Dashboards\nScreenshot needed: Interacting with a live dashboard\n\nData Interaction: Users can interact with the data in various ways, like hovering over charts to see detailed information or clicking on elements to drill down.\nFiltering: Apply filters to refine the data displayed on the entire dashboard or specific charts.\n\n\n\nSharing and Exporting\nScreenshot needed: Sharing and exporting options for a dashboard\n\nSharing Dashboards: Dashboards can be shared with others through direct links or by providing access within Superset.\nExporting: Dashboards can also be exported for external use, either as images or in other formats, depending on Ortege Studio’s configuration.\n\n\n\nSecurity and Permissions\nScreenshot needed: Dashboard’s security settings\n\nAccess Control: You can control who has access to view or edit the dashboard, ensuring data security and integrity.\nRow-Level Security: If set up, row-level security will apply to all the charts in the dashboard, ensuring users only see the data they are authorized to view.\n\n\n\nConclusion\nDashboards in Ortege Studio are an essential feature for synthesizing complex data into comprehensible and actionable insights. They offer a platform to narrate a data story, combining different visual elements into a cohesive and interactive experience."
  },
  {
    "objectID": "studio/datasets.html",
    "href": "studio/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets\nDatasets in Ortege Studio are foundational elements that represent the structured data you work with. They are usually tables or views from connected databases. Understanding the Datasets feature is key to effectively utilizing Ortege Studio for your data analysis and visualization needs.\n\nAccessing Datasets\nScreenshot needed: Accessing Datasets from the Ortege Studio menu\n\nNavigation: You can access the Datasets feature from the main Ortege Studio menu, typically found under the “Data” section.\nDatasets Page: This page lists all available datasets, showing their names, database connections, and other relevant details.\n\n\n\nDataset Overview\nScreenshot needed: Overview of a Dataset\n\nMetadata: Each dataset listing provides metadata, such as the database it belongs to, the schema, and when it was last updated.\nActions: From here, you can perform actions like editing dataset properties, deleting datasets, or exploring the dataset through visualizations.\n\n\n\nEditing a Dataset\nScreenshot needed: Dataset editing interface\n\nEditing Interface: By selecting a dataset, you enter the editing interface. This area allows you to modify various aspects of the dataset.\nChanging Metadata: You can change the dataset’s descriptive information, like its verbose name, description, and more.\n\n\n\nColumns and Metrics\nScreenshot needed: Columns and Metrics section of a dataset\n\nColumns Tab: This tab shows all the columns of your dataset, including their type and source. You can edit column details, like renaming them, changing data types, or adding calculated columns.\nMetrics Tab: Here, you can define and manage custom metrics that are specific to the dataset, like aggregations or formulas.\n\n\n\nDataset Usage\nScreenshot needed: Visualization using a selected dataset\n\nVisualization: Datasets can be directly used to create various visualizations and dashboards.\nSQL Lab Integration: Datasets also integrate seamlessly with SQL Lab, allowing for advanced querying and exploration.\n\n\n\nSecurity and Permissions\nScreenshot needed: Dataset’s security settings\n\nAccess Control: Ortege Studio enables fine-grained access control on datasets, allowing you to control who can view or edit them.\nRow-Level Security: You can also set up row-level security to ensure users only see data they are permitted to view.\n\n\n\nConclusion\nThe Datasets feature in Ortege Studio is a versatile and crucial component of the platform. It allows for the organized management and utilization of your data within Ortege Studio, facilitating everything from simple explorations to complex visualizations and analyses."
  },
  {
    "objectID": "content/lakehouse/medallian.html",
    "href": "content/lakehouse/medallian.html",
    "title": "Medallian Architecture",
    "section": "",
    "text": "Ortege Lakehouse is designed around the innovative Medallion Architecture, a tiered data management strategy that systematically categorizes data into three distinct layers: Bronze, Silver, and Gold. This architecture facilitates a streamlined progression of data from raw ingestion to refined analytics, ensuring users across various roles and functions can access the most relevant and optimized data for their needs. The structured approach underpinning the Medallion Architecture reflects our commitment to delivering clarity, efficiency, and value in data analysis."
  },
  {
    "objectID": "content/lakehouse/medallian.html#bronze-layer",
    "href": "content/lakehouse/medallian.html#bronze-layer",
    "title": "Medallian Architecture",
    "section": "Bronze Layer:",
    "text": "Bronze Layer:\nThe foundation of the Medallion Architecture, the Bronze Layer, houses raw data directly ingested from diverse sources. This layer prioritizes the authenticity and completeness of data, serving as the primary repository for unprocessed information. Views that reference tables within the Bronze Layer remain in this tier, preserving the raw state of data until specific transformations are applied."
  },
  {
    "objectID": "content/lakehouse/medallian.html#silver-layer",
    "href": "content/lakehouse/medallian.html#silver-layer",
    "title": "Medallian Architecture",
    "section": "Silver Layer:",
    "text": "Silver Layer:\nData transitions to the Silver Layer following initial processing, which includes cleansing, normalization, and structuring. This layer enhances the usability of data, making it more accessible and meaningful for analysis. The Silver Layer acts as a bridge between raw data and advanced analytics, providing a balanced dataset that is both rich in detail and optimized for exploration."
  },
  {
    "objectID": "content/lakehouse/medallian.html#gold-layer",
    "href": "content/lakehouse/medallian.html#gold-layer",
    "title": "Medallian Architecture",
    "section": "Gold Layer:",
    "text": "Gold Layer:\nRepresenting the pinnacle of data refinement, the Gold Layer contains datasets that are highly curated and performance-optimized for specific analytical use cases. Data in this layer is tailored to deliver actionable insights, supporting decision-making processes with precision and speed. The Gold Layer embodies the ultimate goal of data transformation within Ortege Lakehouse, offering users targeted, ready-to-use datasets for in-depth analysis."
  },
  {
    "objectID": "content/lakehouse/silver.html",
    "href": "content/lakehouse/silver.html",
    "title": "Silver Layer",
    "section": "",
    "text": "The Silver layer on Bitcoin will focus on exploding the inputs and outputs from the raw data as well as focus on all the L2’s, Rollups and Sidechains that are building on Bitcoin\n\n\nThe vw_test_sl_outputs table is a structured representation of transaction outputs derived from the raw transactional data in the Bronze Layer of the Ortege Lakehouse. This table breaks down the transaction outputs, which are the results of Bitcoin transactions, into a format that is easier to analyze and query.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier of the transaction from which the output is derived.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes the transaction.\n\n\nblock_date\ndate\nThe date when the block containing the transaction was added to the blockchain.\n\n\nis_coinbase\nboolean\nIndicates whether the transaction output is from a coinbase transaction, signifying new bitcoins entering circulation as a miner’s reward.\n\n\naddress\nstring\nThe Bitcoin address that is the recipient of this output.\n\n\nindex\nbigint\nThe position of this output within the transaction; the first output is indexed as 0.\n\n\nrequired_signatures\nbigint\nThe number of signatures required to spend this output.\n\n\nscript_asm\nstring\nThe assembly notation of the script that dictates how this output can be spent.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the output script.\n\n\ntype\nstring\nThe type of script used in the transaction output (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins held in this output, represented in Satoshis (the smallest unit of bitcoin).\n\n\n\n\n\n\nThe vw_test_sl_inputs table is an expanded view of the inputs from the raw transaction data within the Bronze Layer of the Ortege Lakehouse. This table breaks down individual transaction inputs, providing a granular look at the data that constitutes the transactional elements of the Bitcoin network.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier for the transaction containing this input.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes this transaction.\n\n\nblock_date\ndate\nThe date when the block was added to the blockchain.\n\n\nis_coinbase\nboolean\nA flag indicating whether the input is part of a coinbase transaction.\n\n\naddress\nstring\nThe Bitcoin address associated with this input.\n\n\nindex\nbigint\nThe position of this input within the transaction.\n\n\nrequired_signatures\nbigint\nThe number of required signatures for the transaction this input is part of.\n\n\nscript_asm\nstring\nThe assembly notation of the script used in the input.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the script used in the input.\n\n\nsequence\nbigint\nA sequence number associated with this input, used for replacement transactions.\n\n\nspent_output_index\nbigint\nThe index of the output that this input is spending from.\n\n\nspent_transaction_hash\nstring\nThe hash of the transaction from which this input spends.\n\n\ntype\nstring\nThe type of script used in this input (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins being input, typically matching the value of the spent output.\n\n\n\n\n\n\nThe vw_test_sl_block_miners table is a linkage between the mined blocks in the Bronze Layer of the Ortege Lakehouse and the miners who mined them. This table is the result of joining miner address data with block information to establish which entity mined each block.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nblock_height\nbigint\nThe height of a block within the blockchain.\n\n\nminer_name\nstring\nThe name of the miner or mining pool that mined the block.\n\n\n\n\n\n\nThe transaction output data reflects various operations on the Stacks blockchain. Each row represents a transaction output, described by the following fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nid\nRepresents the transaction hash.\n\n\nblock_height\nThe block height the transaction occurred in.\n\n\nblock_date\nThe date the transaction occurred.\n\n\naddress\nAddress of the sender.\n\n\nindex\nThe sequence number of the output in a transaction, starting at 0.\n\n\nscript_asm\nThe script of the transaction. An OP_RETURN followed by specific codes at index 0 indicates a non-spendable output embedding different types of data.\n\n\nvalue\nThe amount associated with the output, typically zero for OP_RETURN outputs as they are not intended for transfer.\n\n\ntransaction_type\n- vrf - VRF key registrations relating to OP_RETURN 58325e (X2^). - commit - Transactions relating to OP_RETURN 58325b (X2[): Block commit. Every output after the index 0, represents an amount that the Stacks miner commits in Satoshi’s to be paid to Stacks stackers. The final transaction is the unspent change to send back to the miner. - stack-stx - Transaction relating to OP_RETURN 583278 (X2x). - transfer-stx - Transactions relating OP_RETURN 583224 (X2$). - preSTX - Transactions relating to OP_RETURN 583270 (X2p).\n\n\ncommit\nA boolean reflecting true if the value is an amount of Satoshi’s committed to STX miners.\n\n\nburnt\nHighlights transactions that send BTC to the burn address 1111111111111111111114oLvT2, indicating that miners burn BTC."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_test_sl_outputs",
    "href": "content/lakehouse/silver.html#vw_test_sl_outputs",
    "title": "Silver Layer",
    "section": "",
    "text": "The vw_test_sl_outputs table is a structured representation of transaction outputs derived from the raw transactional data in the Bronze Layer of the Ortege Lakehouse. This table breaks down the transaction outputs, which are the results of Bitcoin transactions, into a format that is easier to analyze and query.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier of the transaction from which the output is derived.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes the transaction.\n\n\nblock_date\ndate\nThe date when the block containing the transaction was added to the blockchain.\n\n\nis_coinbase\nboolean\nIndicates whether the transaction output is from a coinbase transaction, signifying new bitcoins entering circulation as a miner’s reward.\n\n\naddress\nstring\nThe Bitcoin address that is the recipient of this output.\n\n\nindex\nbigint\nThe position of this output within the transaction; the first output is indexed as 0.\n\n\nrequired_signatures\nbigint\nThe number of signatures required to spend this output.\n\n\nscript_asm\nstring\nThe assembly notation of the script that dictates how this output can be spent.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the output script.\n\n\ntype\nstring\nThe type of script used in the transaction output (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins held in this output, represented in Satoshis (the smallest unit of bitcoin)."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_test_sl_inputs",
    "href": "content/lakehouse/silver.html#vw_test_sl_inputs",
    "title": "Silver Layer",
    "section": "",
    "text": "The vw_test_sl_inputs table is an expanded view of the inputs from the raw transaction data within the Bronze Layer of the Ortege Lakehouse. This table breaks down individual transaction inputs, providing a granular look at the data that constitutes the transactional elements of the Bitcoin network.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nid\nstring\nThe unique identifier for the transaction containing this input.\n\n\nblock_height\nbigint\nThe height of the block in the blockchain that includes this transaction.\n\n\nblock_date\ndate\nThe date when the block was added to the blockchain.\n\n\nis_coinbase\nboolean\nA flag indicating whether the input is part of a coinbase transaction.\n\n\naddress\nstring\nThe Bitcoin address associated with this input.\n\n\nindex\nbigint\nThe position of this input within the transaction.\n\n\nrequired_signatures\nbigint\nThe number of required signatures for the transaction this input is part of.\n\n\nscript_asm\nstring\nThe assembly notation of the script used in the input.\n\n\nscript_hex\nstring\nThe hexadecimal encoding of the script used in the input.\n\n\nsequence\nbigint\nA sequence number associated with this input, used for replacement transactions.\n\n\nspent_output_index\nbigint\nThe index of the output that this input is spending from.\n\n\nspent_transaction_hash\nstring\nThe hash of the transaction from which this input spends.\n\n\ntype\nstring\nThe type of script used in this input (e.g., ‘pubkeyhash’, ‘scripthash’).\n\n\nvalue\nbigint\nThe value of bitcoins being input, typically matching the value of the spent output."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_test_sl_block_miners",
    "href": "content/lakehouse/silver.html#vw_test_sl_block_miners",
    "title": "Silver Layer",
    "section": "",
    "text": "The vw_test_sl_block_miners table is a linkage between the mined blocks in the Bronze Layer of the Ortege Lakehouse and the miners who mined them. This table is the result of joining miner address data with block information to establish which entity mined each block.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nblock_height\nbigint\nThe height of a block within the blockchain.\n\n\nminer_name\nstring\nThe name of the miner or mining pool that mined the block."
  },
  {
    "objectID": "content/lakehouse/silver.html#tbl_test_sl_l2_stacks",
    "href": "content/lakehouse/silver.html#tbl_test_sl_l2_stacks",
    "title": "Silver Layer",
    "section": "",
    "text": "The transaction output data reflects various operations on the Stacks blockchain. Each row represents a transaction output, described by the following fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nid\nRepresents the transaction hash.\n\n\nblock_height\nThe block height the transaction occurred in.\n\n\nblock_date\nThe date the transaction occurred.\n\n\naddress\nAddress of the sender.\n\n\nindex\nThe sequence number of the output in a transaction, starting at 0.\n\n\nscript_asm\nThe script of the transaction. An OP_RETURN followed by specific codes at index 0 indicates a non-spendable output embedding different types of data.\n\n\nvalue\nThe amount associated with the output, typically zero for OP_RETURN outputs as they are not intended for transfer.\n\n\ntransaction_type\n- vrf - VRF key registrations relating to OP_RETURN 58325e (X2^). - commit - Transactions relating to OP_RETURN 58325b (X2[): Block commit. Every output after the index 0, represents an amount that the Stacks miner commits in Satoshi’s to be paid to Stacks stackers. The final transaction is the unspent change to send back to the miner. - stack-stx - Transaction relating to OP_RETURN 583278 (X2x). - transfer-stx - Transactions relating OP_RETURN 583224 (X2$). - preSTX - Transactions relating to OP_RETURN 583270 (X2p).\n\n\ncommit\nA boolean reflecting true if the value is an amount of Satoshi’s committed to STX miners.\n\n\nburnt\nHighlights transactions that send BTC to the burn address 1111111111111111111114oLvT2, indicating that miners burn BTC."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_pox_individual_stackers",
    "href": "content/lakehouse/silver.html#vw_dev_sl_pox_individual_stackers",
    "title": "Silver Layer",
    "section": "vw_dev_sl_pox_individual_stackers",
    "text": "vw_dev_sl_pox_individual_stackers\nTracks various Proof of Transfer (PoX) and Stacking operations on the Stacks blockchain. It includes direct stacking transactions. The transactions are filtered to only include successful ones pertaining to the pox-3 smart contract and are related to stacking operations such as stack-stx, stack-extend, and stack-increase.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nto_timestamp(block_timestamp)\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call\nAn object containing details of the contract call, including function name, contract ID, signature, and arguments.\n\n\ncontract_call.function_name\nThe name of the function called within the contract, indicating the type of stacking operation performed.\n\n\ncontract_call.contract_id\nThe ID of the contract that includes the function being called.\n\n\ncontract_call.function_signature\nThe signature of the function being called in the contract, representing the function’s interface.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\namount (conditional)\nThe amount of STX locked for stacking, normalized to STX units (divided by 1e6), depending on the stacking operation.\n\n\nbtcAddress_to_receive_rewards\nThe Bitcoin address set to receive stacking rewards, extracted and decoded from the function arguments.\n\n\nlock_period (conditional)\nFor stack-stx operations, this represents the number of cycles the tokens are locked for stacking.\n\n\nstart_burn_height (conditional)\nFor stack-stx operations, this is the burnchain block height at which the stacking period starts.\n\n\nextend_cycle (conditional)\nFor stack-extend operations, this signifies the additional number of cycles for which an existing stack is extended."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_pox_delegates",
    "href": "content/lakehouse/silver.html#vw_dev_sl_pox_delegates",
    "title": "Silver Layer",
    "section": "vw_dev_sl_pox_delegates",
    "text": "vw_dev_sl_pox_delegates\nThis dataset focuses on delegated stacking operations, capturing the delegation of STX tokens for stacking purposes. It tracks the delegation transactions, detailing the delegators, delegatees, and the amounts involved.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nblock_timestamp\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call\nAn object containing details of the contract call, including function name, contract ID, signature, and arguments.\n\n\ncontract_call.function_name\nThe name of the function called within the contract, indicating the type of delegated stacking operation.\n\n\ncontract_call.contract_id\nThe ID of the contract that includes the function being called.\n\n\ncontract_call.function_signature\nThe signature of the function being called in the contract, representing the function’s interface.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\namount (conditional)\nThe amount of STX delegated for stacking, specified in the function arguments, depending on the operation type.\n\n\ndelegate_to (conditional)\nThe address to which the STX tokens are being delegated for stacking purposes."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_pox_aggregates",
    "href": "content/lakehouse/silver.html#vw_dev_sl_pox_aggregates",
    "title": "Silver Layer",
    "section": "vw_dev_sl_pox_aggregates",
    "text": "vw_dev_sl_pox_aggregates\nThis dataset captures transactions pertaining to partial stacking commitments, allowing for an analysis of stacking operations that aggregate STX tokens from multiple sources. It provides insights into the delegation process, the amount of STX committed, and the intended reward cycles.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nblock_timestamp\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\ncontract_call\nAn object containing details of the contract call, including function name, contract ID, and signature.\n\n\ncontract_call.function_name\nThe specific function called within the contract, indicating the type of stacking operation.\n\n\ncontract_call.contract_id\nThe ID of the contract that includes the function being called.\n\n\ncontract_call.function_signature\nThe signature of the function being called in the contract.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\namount\nThe amount of microSTX committed to stacking, extracted from the contract log’s event data.\n\n\ndelegator\nThe address of the delegator, if applicable, extracted from the contract log’s event data.\n\n\npox_address\nThe Bitcoin address intended to receive the stacking rewards, extracted from the contract log’s event data.\n\n\nreward_cycle\nThe specific reward cycle(s) for which the stacking commitment is made, extracted from the contract log’s event data.\n\n\nstacker\nThe address of the stacker, extracted from the contract log’s event data."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_bns.sql",
    "href": "content/lakehouse/silver.html#vw_dev_sl_bns.sql",
    "title": "Silver Layer",
    "section": "vw_dev_sl_bns.sql",
    "text": "vw_dev_sl_bns.sql\nThe view retrieves successful transactions related to domain names, such as preorders, registrations, transfers, renewals, and other updates. It aims to offer insights into the usage and activities around the Stacks domain name services.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nto_timestamp(block_timestamp)\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\ncontract_call.function_name\nThe specific function called within the bns contract, indicating the type of domain name operation.\n\n\namount\nConditionally shows the asset amount for name-preorder and the renewal fee for name-renewal.\n\n\nname\nFor name-register, name-transfer, and name-renewal, this is the domain name involved.\n\n\nnew_owner\nFor name-transfer operations, this indicates the new owner’s address of the domain name."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_sip10_tokens",
    "href": "content/lakehouse/silver.html#vw_dev_sl_sip10_tokens",
    "title": "Silver Layer",
    "section": "vw_dev_sl_sip10_tokens",
    "text": "vw_dev_sl_sip10_tokens\nThe vw_dev_sl_sip10_tokens view provides metadata about all tokens that comply with the SIP010 Fungible Token standard. This includes critical information such as the token address and its decimal precision, which is essential for accurately representing token quantities and facilitating token-related operations.\n\n\n\nColumn\nDescription\n\n\n\n\naddress\naddress of the SIP010 token\n\n\ndecimals\nthe amount of decimals of the token"
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_bridge_abtc",
    "href": "content/lakehouse/silver.html#vw_dev_sl_bridge_abtc",
    "title": "Silver Layer",
    "section": "vw_dev_sl_bridge_abtc",
    "text": "vw_dev_sl_bridge_abtc\nThis dataset captures successful transaction events for ABTC. The dataset includes transactions for transfer, mint-fixed, and burn-fixed operations. Each record details the transaction hash, sender address, block number, timestamp of the block, the events array, the name of the function called in the contract (function_name), and the arguments passed to the function (function_args). Additionally, it extracts the amount for transfer, burn-fixed, and mint-fixed functions, and the recipient for transfer function.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\nto_timestamp(block_timestamp)\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call.function_name\nThe specific function called within the contract, indicating the type of token operation performed.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\namount (conditional)\nThe token amount involved in the transfer, mint-fixed, or burn-fixed operations, extracted from the function arguments.\n\n\nrecipient (conditional)\nFor transfer operations, the address of the recipient of the tokens, extracted from the function arguments."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_stx20",
    "href": "content/lakehouse/silver.html#vw_dev_sl_stx20",
    "title": "Silver Layer",
    "section": "vw_dev_sl_stx20",
    "text": "vw_dev_sl_stx20\nThe vw_dev_sl_stx20 view captures transactions involving STX20 tokens, including minting, deploying, and transferring operations. It includes transaction identifiers, participant addresses, transaction dates, and operation types, among other details. This view is vital for tracking STX20 token movements and understanding their supply dynamics.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nhash\nUnique identifier for a transaction (varchar)\n\n\nsender_address\nAddress of the sender (varchar)\n\n\nfee_rate\nTransaction fee rate (varchar)\n\n\nrecipient_address\nAddress of the recipient (varchar)\n\n\nstx_paid\nAmount of STX paid (varchar)\n\n\ndate\nDate of the transaction (date)\n\n\nmemo_text\nText of the memo field (varchar)\n\n\nSTX_operation\nType of operation (mint, deploy, or transfer) (enum)\n\n\nstx20_ticker\nTicker symbol for the token (varchar)\n\n\namount\nAmount of tokens (varchar)\n\n\ntotal_supply\nTotal supply of tokens (varchar)\n\n\nlimit_per_mint\nLimit per mint operation (varchar)"
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_dapp_alex_swap",
    "href": "content/lakehouse/silver.html#vw_dev_sl_dapp_alex_swap",
    "title": "Silver Layer",
    "section": "vw_dev_sl_dapp_alex_swap",
    "text": "vw_dev_sl_dapp_alex_swap\nThe vw_dev_sl_dapp_alex_swap view provides a comprehensive look into the smart contract activity of the ALEX dApp, showcasing token swaps. It includes transaction timestamps, block numbers, contract identifiers, and detailed function arguments, facilitating an in-depth analysis of dApp interactions on the Stacks blockchain.\n\n\n\nColumn\nDescription\n\n\n\n\nblock_timestamp\nTimestamp of the block (datetime)\n\n\nblock_number\nNumber of the block (bigint)\n\n\nhash\nUnique identifier for a transaction (varchar)\n\n\nsender_address\nAddress of the sender (varchar)\n\n\ncontract_id\nIdentifier of the contract (varchar)\n\n\nfunction_name\nName of the function called (varchar)\n\n\nfunction_args\nArguments passed to the function (struct)\n\n\ntokenA\nIdentifier for token A (varchar)\n\n\ntokenB\nIdentifier for token B (varchar)\n\n\ntokenAOut\nAmount of token A output (bigint)\n\n\ntokenBIn\nAmount of token B input (varchar)\n\n\n\nPlease note This chart needs some improvements which will be found in the Gold Layer. Take note the volume has all been divided by 8 decimals places. This is hardcoded and some of those tokens may not have 8 decimal places so to get accurate figures you’d need to get the decimals from vw_dev_sl_sip10_tokens"
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_dapp_stacking_dao",
    "href": "content/lakehouse/silver.html#vw_dev_sl_dapp_stacking_dao",
    "title": "Silver Layer",
    "section": "vw_dev_sl_dapp_stacking_dao",
    "text": "vw_dev_sl_dapp_stacking_dao\nThis dataset includes successful transaction records for staking-related operations (deposit, withdraw, init-withdraw, add-rewards) on the Stacks blockchain. Each record in the dataset includes the transaction hash, sender address, block number, and a human-readable transaction date. It also details the events triggered, the specific function called within the staking contract, and the arguments provided to the function call. The dataset captures the amount involved in the transactions and, for certain operations, the NFT number and cycle number related to the transaction.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\ntx_date\nThe timestamp of the block, converted to a human-readable date and time format.\n\n\nevents\nAn array of events triggered by the transaction.\n\n\ncontract_call.function_name\nThe specific function called within the staking contract.\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\ntx_result\nThe result of the transaction execution, containing details about the success or failure of the operation.\n\n\ntx_status\nThe status of the transaction, indicating whether it was successful.\n\n\namount (conditional)\nThe token amount involved in the deposit, init-withdraw, withdraw, or add-rewards operations, based on the function name.\n\n\nnft_number (conditional)\nThe NFT number associated with the init-withdraw and withdraw operations.\n\n\ncycle (conditional)\nFor add-rewards operations, this field indicates the cycle number for which rewards are being added."
  },
  {
    "objectID": "content/lakehouse/silver.html#vw_dev_sl_dapp_bitflow",
    "href": "content/lakehouse/silver.html#vw_dev_sl_dapp_bitflow",
    "title": "Silver Layer",
    "section": "vw_dev_sl_dapp_bitflow",
    "text": "vw_dev_sl_dapp_bitflow\nThis dataset focuses on successful liquidity and swap event transactions. It covers add-liquidity, swap-x-for-y, swap-y-for-x, and withdraw-liquidity operations. Each record provides a transaction hash, the sender’s address, the block number, the date and time of the transaction (tx_date), an array of events (events), the function name indicating the operation (contract_call.function_name), and the function arguments (contract_call.function_args). The dataset also includes calculated fields for the amounts of token X and token Y involved in each transaction, with positive values typically representing token acquisitions and negative values indicating disbursements or deductions.\n\n\n\n\n\n\n\nField Name\nDescription\n\n\n\n\nhash\nThe unique identifier of the transaction on the blockchain.\n\n\nsender_address\nThe Stacks wallet address of the sender initiating the transaction.\n\n\nblock_number\nThe block number in which the transaction was included.\n\n\ntx_date\nThe timestamp of the block, formatted as a human-readable date and time.\n\n\nevents\nAn array of events that were triggered by the transaction.\n\n\ncontract_call.function_name\nThe name of the function called, indicating the type of operation (e.g., adding liquidity, swapping, etc.).\n\n\ncontract_call.function_args\nThe arguments provided to the function call within the contract.\n\n\ntokenY\nRepresents the identifier or amount of token Y involved, extracted from the first function argument.\n\n\ntokenX_amount (conditional)\nThe calculated amount of token X involved in the operation, depending on the function name.\n\n\ntokenY_amount (conditional)\nThe calculated amount of token Y involved in the operation, depending on the function name."
  },
  {
    "objectID": "content/lakehouse/bronze.html",
    "href": "content/lakehouse/bronze.html",
    "title": "Bronze Layer",
    "section": "",
    "text": "Welcome to the testing phase of the Ortege Lakehouse, where the Bronze Layer acts as the primary reservoir for raw data from varied sources. This layer serves as the initial platform for data analysis and processing within the Ortege ecosystem. It’s essential to recognize the testing environment’s relative stability, which nonetheless permits periodic updates and modifications for improvement.\n\n\nThe data in the Bronze Layer during testing is more stable than in active development but still flexible to accommodate enhancements:\n\nIterative Updates: Expect incremental changes as we refine data sets and adapt to new insights.\nSchema Adaptability: Data structures may be adjusted to align with evolving requirements and findings.\nQuality Progression: Data quality is continually advancing towards our high standards, with interim variations as part of the testing phase.\n\n\n\n\nTo make the most of the Bronze Layer during testing:\n\nStay Updated: Follow the latest changes through our communications and documentation.\nExplore and Test: Engage with the data, keeping in mind the environment’s purpose to test and improve.\nFeedback Loop: Contribute feedback to aid in the evolution of data management practices.\n\nEmbrace this phase as an integral step toward a robust and sophisticated data infrastructure."
  },
  {
    "objectID": "content/lakehouse/bronze.html#managing-expectations-in-testing",
    "href": "content/lakehouse/bronze.html#managing-expectations-in-testing",
    "title": "Bronze Layer",
    "section": "",
    "text": "The data in the Bronze Layer during testing is more stable than in active development but still flexible to accommodate enhancements:\n\nIterative Updates: Expect incremental changes as we refine data sets and adapt to new insights.\nSchema Adaptability: Data structures may be adjusted to align with evolving requirements and findings.\nQuality Progression: Data quality is continually advancing towards our high standards, with interim variations as part of the testing phase."
  },
  {
    "objectID": "content/lakehouse/bronze.html#utilizing-the-testing-environment",
    "href": "content/lakehouse/bronze.html#utilizing-the-testing-environment",
    "title": "Bronze Layer",
    "section": "",
    "text": "To make the most of the Bronze Layer during testing:\n\nStay Updated: Follow the latest changes through our communications and documentation.\nExplore and Test: Engage with the data, keeping in mind the environment’s purpose to test and improve.\nFeedback Loop: Contribute feedback to aid in the evolution of data management practices.\n\nEmbrace this phase as an integral step toward a robust and sophisticated data infrastructure."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_dev_br_blocks",
    "href": "content/lakehouse/bronze.html#tbl_dev_br_blocks",
    "title": "Bronze Layer",
    "section": "tbl_dev_br_blocks",
    "text": "tbl_dev_br_blocks\nThis table encapsulates comprehensive details on Bitcoin blocks, encompassing mining data, block identification, and transaction specifics. It records the difficulty level, mining rewards, the unique hash identifying each block, its position within the blockchain, and the Merkle root for transaction integrity verification. Additionally, it includes metrics such as the block’s size, the total number of transactions it contains, and the cumulative work or “chainwork” up to that point, expressed in hexadecimal. This table not only provides insights into the mining process, including the nonce and the compact target “bits,” but also captures the economic aspects of mining through fields like mint reward, total fees, and the overall reward. The tbl_dev_br_blocks serves as a pivotal resource for analyzing the blockchain’s structural and economic dynamics, offering a granular view of each block’s contribution to the ledger.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nbits\nstring\nThe encoded form of the target threshold this block’s header hash must be less than or equal to.\n\n\nchainwork\nstring\nThe total amount of work done on this block’s chain, in hexadecimal.\n\n\ncoinbase\nstring\nThe transaction containing the miner’s reward and any fees paid by transactions included in this block.\n\n\ndate\nstring\nThe date and time when this block was mined.\n\n\ndifficulty\nstring\nThe difficulty target for this block.\n\n\nhash\nstring\nThe hash of this block’s header.\n\n\nheight\nbigint\nThe height of this block in the blockchain.\n\n\nmerkle_root\nstring\nThe Merkle root of the transactions included in this block.\n\n\nmint_reward\ndouble\nThe total amount of new bitcoins generated by mining this block.\n\n\nnonce\nstring\nA random number used to try to find a valid block hash.\n\n\nprevious_block_hash\nstring\nThe hash of the previous block in the blockchain.\n\n\nsize\nbigint\nThe size of this block in bytes.\n\n\nstripped_size\nbigint\nThe size of this block without witness data in bytes.\n\n\ntime\ntimestamp\nThe Unix timestamp when this block was mined.\n\n\ntotal_fees\ndouble\nThe total amount of fees paid by transactions included in this block.\n\n\ntotal_reward\ndouble\nThe total amount of bitcoins generated by mining this block, including both the mint reward and transaction fees.\n\n\ntransaction_count\nbigint\nThe number of transactions included in this block.\n\n\nweight\nbigint\nThe weight of this block, used to calculate transaction fees."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_dev_br_transactions",
    "href": "content/lakehouse/bronze.html#tbl_dev_br_transactions",
    "title": "Bronze Layer",
    "section": "tbl_dev_br_transactions",
    "text": "tbl_dev_br_transactions\nThis table provides a detailed ledger of transactions within the Bitcoin chain, capturing both the technical aspects and the economic activities of each transaction. It records the transaction’s metadata, such as its unique identifier (id), the block_hash it belongs to, its block_height and timing (block_date and block_time), and its placement within the block (index). The table delves into the composition of transactions, detailing the number and total value of inputs (input_count and input_value) and outputs (output_count and output_value), alongside arrays of input and output objects for granular transaction analysis. It also highlights whether a transaction is a coinbase transaction, indicating a miner’s reward (is_coinbase), the transaction fees involved (fee), and technical parameters like hex representation, lock_time, size, version, and virtual_size. This comprehensive transactional overview aids in understanding the flow of bitcoins, transaction costs, and the operational dynamics of the Bitcoin blockchain.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nblock_date\ndate\nThe date when the block was added to the blockchain\n\n\nblock_hash\nstring\nThe unique identifier of the block\n\n\nblock_height\nbigint\nThe height of the block in the blockchain\n\n\nblock_time\ntimestamp\nThe time when the block was added to the blockchain\n\n\ncoinbase\nstring\nThe address that received the block reward for mining the block\n\n\nfee\nbigint\nThe transaction fee paid by the sender\n\n\nhex\nstring\nThe hexadecimal representation of the transaction\n\n\nid\nstring\nThe unique identifier of the transaction\n\n\nindex\nbigint\nThe index of the transaction in the block\n\n\ninput_count\nbigint\nThe number of inputs in the transaction\n\n\ninput_value\nbigint\nThe total value of all inputs in the transaction\n\n\ninputs\narray\nAn array of input objects containing information about each input in the transaction\n\n\nis_coinbase\nboolean\nA boolean indicating whether the transaction is a coinbase transaction\n\n\nlock_time\nbigint\nThe lock time of the transaction\n\n\noutput_count\nbigint\nThe number of outputs in the transaction\n\n\noutput_value\nbigint\nThe total value of all outputs in the transaction\n\n\noutputs\narray\nAn array of output objects containing information about each output in the transaction\n\n\nsize\nbigint\nThe size of the transaction in bytes\n\n\nversion\nbigint\nThe version number of the transaction\n\n\nvirtual_size\nbigint\nThe virtual size of the transaction"
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_test_br_miners",
    "href": "content/lakehouse/bronze.html#tbl_test_br_miners",
    "title": "Bronze Layer",
    "section": "tbl_test_br_miners",
    "text": "tbl_test_br_miners\nThe tbl_test_br_miners table maintains a mapping of blockchain miner addresses to their corresponding identifiable miner names. It is used to associate mining activities with specific entities or mining pools within the Bitcoin network.\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\naddress\nstring\nThe unique Bitcoin address associated with a miner or mining pool.\n\n\nminer\nstring\nThe name of the miner or mining pool associated with the address."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_dev_br_cycles",
    "href": "content/lakehouse/bronze.html#tbl_dev_br_cycles",
    "title": "Bronze Layer",
    "section": "tbl_dev_br_cycles",
    "text": "tbl_dev_br_cycles\nThe tbl_dev_sl_cycles table provides a structured overview of cycles within the Stacks blockchain, starting from the genesis of Stacks 2.0. Each cycle represents a fixed number of blocks, facilitating the organization and analysis of blockchain data over time. This table is essential for understanding the temporal division of blockchain activity and aids in analyzing trends, rewards distributions, and other cycle-based metrics.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ncycle\nThe sequential number of the cycle, starting from 0. Each cycle represents a group of 2100 blocks within the Stacks blockchain.\n\n\nstart_block\nThe starting block number for the given cycle. This marks the first block in the cycle.\n\n\nend_block\nThe ending block number for the given cycle. This marks the last block in the cycle."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_dev_br_blocks-1",
    "href": "content/lakehouse/bronze.html#tbl_dev_br_blocks-1",
    "title": "Bronze Layer",
    "section": "tbl_dev_br_blocks",
    "text": "tbl_dev_br_blocks\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\ncanonical\nboolean\nCanonical Blocks are the building blocks that contribute to the continuous, unbroken chain of blocks in the blockchain, each linking to its predecessor, forming the indelible ledger that blockchain is celebrated for. You expect this to be true.\n\n\nnumber\nbigint\nstacks block number\n\n\nhash\nstring\nA unique identifier for each block, generated by applying a cryptographic hash function to the block’s data\n\n\nindex_block_hash\nstring\nThe hash of the index block.\n\n\nparent_block_hash\nstring\nThe hash of the parent block.\n\n\ntimestamp\nbigint\nTimestamp of relevant block. If you use to_timestamp(timestamp) SQL code, you will get the actual date-time.\n\n\nburn_block_hash\nstring\nCorresponding Bitcoin block number’s hash. The block that is written in Bitcoin block’s hash.\n\n\nburn_block_height\nbigint\nCorresponding Bitcoin block number. The block that is written in a Bitcoin block.\n\n\nminer_txid\nstring\nCorresponding transaction burned on Bitcoin block for the stacks block. Every stacks block is a transaction on Bitcoin. And this is that tx’s id. If you remove the 0x from the beginning, you will get the Bitcoin tx id.\n\n\ntransaction_count\nbigint\nCount of each transaction in the relevant block.\n\n\nexecution_cost_read_count\nbigint\nCaptures the number of independent reads performed on the underlying data store.\n\n\nexecution_cost_read_length\nbigint\nThe number of bytes read from the underlying data store.\n\n\nexecution_cost_runtime\nbigint\nCaptures the number of cycles that a single processor would require to process the Clarity block. This is a unitless metric, meant to provide a basis for comparison between different Clarity code blocks.\n\n\nexecution_cost_write_count\nbigint\nCaptures the number of independent writes performed on the underlying data store (see SIP-004).\n\n\nexecution_cost_write_length\nbigint\nThe number of bytes written to the underlying data store."
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_dev_br_transactions-1",
    "href": "content/lakehouse/bronze.html#tbl_dev_br_transactions-1",
    "title": "Bronze Layer",
    "section": "tbl_dev_br_transactions",
    "text": "tbl_dev_br_transactions\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\nhash\nstring\nUnique identifier for the transaction\n\n\nnonce\nbigint\nRandom number used in mining process\n\n\nfee_rate\nstring\nFee rate for the transaction\n\n\nsender_address\nstring\nAddress of the sender\n\n\nsponsored\nboolean\nBoolean indicating if the transaction is sponsored\n\n\npost_condition_mode\nstring\nMode for post-conditions\n\n\npost_conditions\narray\nArray of post-conditions\n\n\nanchor_mode\nstring\nMode for anchoring\n\n\nis_unanchored\nboolean\nBoolean indicating if the transaction is unanchored\n\n\nparent_block_hash\nstring\nHash of the parent block\n\n\nblock_hash\nstring\nHash of the block containing the transaction\n\n\nblock_number\nbigint\nBlock number containing the transaction\n\n\nblock_timestamp\nbigint\nTimestamp of the block containing the transaction\n\n\nparent_burn_block_time\nbigint\nTimestamp of the parent burn block\n\n\ncanonical\nboolean\nBoolean indicating if the block is canonical\n\n\ntx_index\nbigint\nIndex of the transaction within the block\n\n\ntx_status\nstring\nStatus of the transaction\n\n\ntx_result\nstruct\nResult of the transaction\n\n\nmicroblock_hash\nstring\nHash of the microblock containing the transaction\n\n\nmicroblock_sequence\nbigint\nSequence number of the microblock containing the transaction\n\n\nmicroblock_canonical\nboolean\nBoolean indicating if the microblock is canonical (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nevent_count\nbigint\nCount of events (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nevents\narray\nArray of events (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_read_count\nbigint\nNumber of independent reads performed on the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_read_length\nbigint\nNumber of bytes read from the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_runtime\nbigint\nNumber of cycles for processing (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_write_count\nbigint\nNumber of independent writes performed on the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nexecution_cost_write_length\nbigint\nNumber of bytes written to the underlying data store (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ntx_type\nstring\nType of the transaction (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ntoken_transfer\nstruct\nDetails of token transfer if applicable (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ncoinbase_payload\nstruct\nData for coinbase transactions (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\nsmart_contract\nstruct\nDetails of the smart contract if applicable (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\ncontract_call\nstruct\nDetails of the contract call if applicable (Field marked as ‘null’ suggests missing information or not applicable.)\n\n\npoison_microblock\nmap&lt;string,string&gt;\nInformation on poisoned microblock if applicable (Field marked as ‘null’ suggests missing information or not applicable.)"
  },
  {
    "objectID": "content/lakehouse/bronze.html#tbl_dev_br_contracts",
    "href": "content/lakehouse/bronze.html#tbl_dev_br_contracts",
    "title": "Bronze Layer",
    "section": "tbl_dev_br_contracts",
    "text": "tbl_dev_br_contracts\n\n\n\n\n\n\n\n\ncol_name\ndata_type\ncomment\n\n\n\n\ntx_hash\nstring\nTransaction hash of the contract. Try searching it in explorer.hiro.so\n\n\ncanonical\nboolean\nCanonical Blocks are the building blocks that contribute to the continuous, unbroken chain of blocks in the blockchain, each linking to its predecessor, forming the indelible ledger that blockchain is celebrated for. You expect this to be true.\n\n\naddress\nstring\nAddress of the contract.\n\n\nblock_number\nbigint\nIn which block this contract recorded in.\n\n\nclarity_version\nbigint\nClarity version of the contract.\n\n\nsource_code\nstring\nThe source code of the contract.\n\n\nabi\nstring\nThe ABI (Application Binary Interface) of the contract.\n\n\nis_stx20\nboolean\nIndicates whether the contract is a Stacks 2.0 token (STX20).\n\n\nis_nft\nboolean\nIndicates whether the contract is a non-fungible token (NFT)."
  },
  {
    "objectID": "content/studio/dashboards.html",
    "href": "content/studio/dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "Dashboards in Ortege Studio\nDashboards are powerful tools within Ortege Studio that allow users to compile and visualize various charts and widgets into a single, cohesive interface. Designed for flexibility and interactivity, dashboards provide a snapshot of key metrics, trends, and insights, enabling data-driven decision-making at a glance. This section of the documentation will guide you through creating, customizing, and sharing dashboards in Ortege Studio.\n\n\nCreating a Dashboard\n\nInitiate Dashboard Creation: Navigate to the Dashboard section and click on the “Create” button. You’ll be prompted to choose between starting from scratch or duplicating an existing dashboard.\nAdd Charts: Once your dashboard shell is created, add charts by clicking the “+ Add Chart” button. You can select from existing charts or create new ones to add to the dashboard.\nArrange Layout: Drag and drop charts to arrange them on your dashboard. You can adjust the size and position of each chart to create a visually appealing layout.\n\n\n\nCustomizing Dashboards\n\nEdit Properties: Access the dashboard’s properties to rename it, add a description, or update its settings. This is where you can make the dashboard public or private and set refresh intervals.\nApply Filters: Enhance dashboard interactivity by adding filters. Filters allow viewers to refine what data is displayed across the entire dashboard, making it more dynamic and useful.\nStyling: Use the styling options to apply your branding or preferred visual theme to the dashboard. You can adjust colors, fonts, and spacing to match your organization’s style guide.\n\n\n\nSharing Dashboards\n\nDirect Sharing: Share a dashboard directly with other Ortege Studio users by granting them access. You can control whether they can view, edit, or manage the dashboard.\nPublic Link: Generate a public link to your dashboard for easy sharing with external stakeholders. Ensure sensitive data is appropriately protected when using this feature.\nExporting: Dashboards can be exported as images or PDF documents for offline viewing or inclusion in reports and presentations.\n\n\n\nBest Practices for Dashboard Design\n\nFocus on Clarity: Ensure your dashboards are easy to understand at a glance. Use clear chart titles and labels, and avoid overcrowding the dashboard with too many charts.\nLogical Grouping: Group related charts close to each other to tell a cohesive story. This helps users quickly make connections between different data points.\nConsistent Styling: Apply a consistent style across all charts for a professional appearance. Consistency in colors, fonts, and chart types improves readability.\n\n\n\nMaintaining Dashboards\n\nRegular Updates: Keep your dashboards relevant by regularly updating the charts and data they display. Consider setting up automatic data refreshes if supported.\nGather Feedback: Engage with your dashboard viewers to collect feedback and identify areas for improvement. User input can guide you in refining and enhancing your dashboards.\n\nDashboards are a central feature of Ortege Studio, offering a powerful way to present and explore data. By following these guidelines, you can create effective and impactful dashboards that serve as valuable tools for your organization’s data analysis and decision-making processes."
  },
  {
    "objectID": "content/studio/charts.html",
    "href": "content/studio/charts.html",
    "title": "Charts",
    "section": "",
    "text": "Charts in Ortege Studio\nCharts are at the heart of data visualization in Ortege Studio, offering a dynamic way to represent data and uncover insights through a wide range of visual formats. This section of the documentation will walk you through the process of creating, customizing, and managing charts, ensuring you can fully leverage Ortege Studio’s powerful visualization capabilities.\n\n\nCreating a Chart\n\nSelect Dataset: Begin by choosing the dataset from which you want to create your chart. Ortege Studio supports a diverse array of datasets, ensuring you can visualize data from various sources.\nChoose Chart Type: Select the type of chart that best fits your data visualization needs. Ortege Studio offers an extensive selection, from traditional bar and line charts to more complex scatter plots and heatmaps.\nConfigure Chart: Customize your chart by specifying dimensions and metrics, applying filters, and setting aggregation functions. This step is crucial in defining how your data will be displayed.\n\n\n\nCustomizing Charts\n\nStyling Options: Use the chart’s styling options to enhance its appearance. You can adjust colors, fonts, axis labels, and legends to make your chart more informative and visually appealing.\nInteractive Features: Enhance user interaction by enabling tooltips, clickable elements, and drill-down capabilities. These features make your charts more engaging and informative.\nResponsive Design: Ensure your charts are responsive and look great on all devices. Ortege Studio allows you to preview and adjust chart layouts for different screen sizes.\n\n\n\nManaging Charts\n\nSaving and Organizing: Once you are satisfied with your chart, save it to your dashboard or a specific folder. Proper organization helps in easily accessing and managing your charts.\nSharing and Collaboration: Share your charts with team members for collaborative analysis or embed them in presentations and reports. Ortege Studio provides various sharing options, including direct links and embedding codes.\nVersion Control: Keep track of changes to your charts with version control. This feature allows you to revert to previous versions if needed and understand how your chart evolved over time.\n\n\n\nBest Practices for Chart Design\n\nSimplicity is Key: Aim for clarity and simplicity in your charts. Avoid unnecessary decorations that can distract from the data.\nChoose the Right Chart Type: Match the chart type with the nature of your data and the story you want to tell. Each chart type serves a specific purpose.\nUse Color Wisely: Use color to enhance comprehension, not just for aesthetics. Differentiate data points clearly and use consistent color schemes across similar charts.\n\n\n\nAdvanced Chart Features\n\nCustom SQL and Expressions: For advanced users, Ortege Studio supports the use of custom SQL queries and expressions to create more complex data visualizations.\nIntegration with Predictive Analytics: Leverage Ortege Studio’s integration capabilities to incorporate predictive analytics into your charts, providing forward-looking insights based on historical data trends.\nAPI Access: Automate chart creation and updates through Ortege Studio’s API, enabling dynamic and real-time data visualizations in your charts.\n\nCharts are a fundamental aspect of data visualization in Ortege Studio, enabling users to tell compelling stories with their data. By understanding how to effectively create, customize, and manage your charts, you can unlock the full potential of your data, providing valuable insights to drive decision-making within your organization."
  },
  {
    "objectID": "content/studio/datasets.html",
    "href": "content/studio/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets in Ortege Studio\nDatasets form the backbone of any analysis in Ortege Studio, serving as the structured collection of data upon which charts, dashboards, and insights are built. This section of the documentation will guide you through managing datasets, including how to import, prepare, and optimize them for analysis within Ortege Studio.\n\n\nImporting Datasets\n\nData Sources: Ortege Studio allows you to connect to a wide range of data sources, including relational databases, cloud storage solutions, and real-time data streams. Identify the source of your data to begin the import process.\nImport Process: Use the data import wizard in Ortege Studio to guide you through the steps of connecting to your data source and importing your dataset. This process includes specifying connection details, selecting the data to import, and defining import settings.\nValidation and Preview: Once your data is imported, validate and preview the dataset to ensure accuracy and completeness. This step is crucial for catching any issues before moving on to analysis.\n\n\n\nPreparing Datasets\n\nCleaning Data: Ortege Studio provides tools for cleaning and preprocessing your data, such as removing duplicates, handling missing values, and correcting data types. Clean data is essential for accurate analysis.\nDefining Metrics and Dimensions: Identify and define the key metrics and dimensions within your dataset. This categorization is critical for creating meaningful charts and dashboards later on.\nData Transformation: Apply transformations to your dataset to prepare it for analysis. This can include aggregating data, creating calculated fields, and applying filters.\n\n\n\nOptimizing Datasets\n\nPerformance Tuning: Large datasets can impact the performance of your visualizations. Use Ortege Studio’s optimization features, such as indexing and materialized views, to improve query performance and speed up load times.\nData Modeling: For complex analyses, consider building a data model within Ortege Studio. A well-structured data model can simplify analysis and improve performance by efficiently organizing data relationships.\nData Caching: Leverage Ortege Studio’s data caching capabilities to store frequently accessed data in memory, reducing the need for repeated queries to the data source and accelerating data retrieval.\n\n\n\nBest Practices for Dataset Management\n\nRegular Updates: Keep your datasets up to date with regular refreshes. This ensures your analyses are based on the most current data available.\nSecurity and Privacy: Implement data security and privacy measures to protect sensitive information. This includes managing access controls and anonymizing personal data where necessary.\nDocumentation: Document your datasets, including sources, transformations applied, and any known limitations or issues. Good documentation supports effective collaboration and analysis.\n\n\n\nAdvanced Dataset Features\n\nVersion Control: Maintain versions of your datasets to track changes over time. This is especially useful for datasets that undergo frequent updates or changes.\nCollaborative Editing: Ortege Studio supports collaborative dataset management, allowing multiple users to work on a dataset simultaneously. This feature facilitates teamwork and ensures consistency across analyses.\nIntegration with Machine Learning: Prepare and optimize your datasets for machine learning models. Ortege Studio’s integration with predictive analytics tools enables you to enrich your analyses with advanced data modeling techniques.\n\nDatasets are a critical component of the analytical process in Ortege Studio, enabling users to derive actionable insights from their data. By effectively managing your datasets—from import and preparation to optimization—you lay the groundwork for powerful data analysis and visualization within your organization."
  },
  {
    "objectID": "content/ortegegpt/overview.html",
    "href": "content/ortegegpt/overview.html",
    "title": "OrtegeGPT : Revolutionizing Blockchain Data Analysis",
    "section": "",
    "text": "Introduction\nOrtegeGPT represents the forefront of innovation in blockchain analytics, embodying our commitment to harnessing the power of advanced artificial intelligence to transform data interaction. Currently under rigorous development, OrtegeGPT is poised to become a pivotal tool in the realm of blockchain data analysis, leveraging the capabilities of Large Language Models (LLMs) to offer an unparalleled user experience.\n\n\nVision\nOur vision for OrtegeGPT is to democratize access to complex blockchain datasets, enabling users to query, analyze, and gain insights from blockchain data through intuitive, natural language commands. By integrating cutting-edge AI with our deep expertise in blockchain technology, OrtegeGPT aims to simplify the complexities of blockchain data, making it accessible to analysts, developers, and enthusiasts alike.\n\n\nDevelopment Status\nOrtegeGPT is currently in heavy development, with our team dedicated to refining its capabilities, ensuring robustness, and tailoring the platform to meet the nuanced needs of blockchain analytics. Our approach combines rigorous testing with continuous feedback loops to create a tool that not only meets but exceeds the expectations of our users.\n\n\nFeatures to Anticipate\nWhile we are keeping some details under wraps until closer to release, users can look forward to the following features in OrtegeGPT:\n\nReal-Time Querying: Instantly query blockchain data across multiple platforms using natural language, making data analysis more accessible and efficient.\nPredictive Analytics: Leverage the predictive power of OrtegeGPT to forecast trends, identify patterns, and make data-driven decisions in the blockchain space.\nCustom Data Integration: Seamlessly integrate your proprietary data with OrtegeGPT, enriching your analysis and insights.\nMultiple Distribution Channels: Access OrtegeGPT through various channels, including API access, web interface, and integrated solutions within Ortege Studio, ensuring flexibility and ease of use.\n\n\n\nLooking Ahead\nAs we continue to develop OrtegeGPT, our focus remains on creating a tool that will not only revolutionize how we interact with blockchain data but also empower our users to unlock new insights and drive innovation. We are excited about the future of OrtegeGPT and look forward to sharing more details with you soon.\n\n\nStay Informed\nWe understand the anticipation surrounding OrtegeGPT and are committed to keeping our community informed. Updates on development progress, feature highlights, and eventual release details will be shared through our official channels. Stay tuned for an exciting journey ahead with OrtegeGPT."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Ortege",
    "section": "",
    "text": "Ortege Studio\nOrtege Studio: Ortege Studio is a cutting-edge analytics platform designed to empower users with unparalleled insights into their data through an extensive array of visualization options, with over 30 types of charts available to cater to a wide range of analytical needs. At its core, Ortege Studio harnesses the power of predictive analytics, enabling businesses to forecast trends and make data-driven decisions with greater accuracy. The platform stands out by offering the flexibility to connect to a multitude of data sources, allowing users to seamlessly integrate and harmonize disparate data sets. Thanks to its sophisticated Data Manipulation Language (DML) capabilities, users can effortlessly join, query, and analyze their data across various sources, all within a user-friendly interface. Ortege Studio is the ultimate tool for organizations looking to unlock the full potential of their data, offering a comprehensive solution that blends advanced analytics with intuitive data integration and exploration features.\n\n\nOrtege Lakehouse\nOrtege Lakehouse: Ortege Lakehouse is an innovative data management platform engineered for the era of big data, embodying cutting-edge technology to meet the demands of extensive datasets and complex analytics. Designed to effortlessly integrate with MLOps frameworks and Large Language Models (LLMs), it provides a robust foundation for advanced machine learning projects and predictive analytics. A standout feature of Ortege Lakehouse is its comprehensive datasets for a wide array of the market’s most popular blockchains. With these datasets, users gain immediate access to a wealth of blockchain data, streamlined for analysis. Moreover, our platform introduces “silver and gold layers” – a feature that simplifies the querying of complex blockchain datasets, ensuring that even the most intricate data becomes easily navigable. This makes Ortege Lakehouse an indispensable tool for organizations looking to leverage blockchain data for insightful analysis, predictive modeling, and strategic decision-making, all while ensuring seamless workflow integration and operational efficiency.\n\n\nOrtegeGPT\nOrtegeGPT: OrtegeGPT, our latest innovation currently under rigorous development, is poised to redefine the way we interact with blockchain data. This cutting-edge Large Language Model (LLM) is being meticulously crafted to provide real-time, intuitive querying capabilities across a vast spectrum of supported blockchains. Upon completion, OrtegeGPT will empower users to effortlessly extract insights, perform complex analyses, and obtain answers to virtually any query about the blockchain ecosystems we support. Recognizing the diverse needs of our users, OrtegeGPT is also designed with flexibility in mind, allowing organizations to integrate their proprietary data seamlessly, thereby enriching their analytical capabilities and insights. Furthermore, OrtegeGPT is set to offer multiple distribution channels, ensuring that our advanced LLM can be accessed conveniently, whether through direct API integrations, web interfaces, or customized applications. This ambitious feature signifies our commitment to pioneering in the blockchain analytics space, offering an unparalleled tool that combines the latest in AI technology with the expansive world of blockchain data."
  },
  {
    "objectID": "content/studio/sqllab.html",
    "href": "content/studio/sqllab.html",
    "title": "SQL Lab",
    "section": "",
    "text": "SQL Lab in Ortege Studio\nSQL Lab is a powerful feature within Ortege Studio that caters to data analysts and engineers who prefer to interact with their data through SQL queries. This integrated development environment (IDE) is designed for crafting, testing, and visualizing SQL queries in real-time, providing a direct and flexible approach to data exploration. This section of the documentation will introduce SQL Lab, detailing its key functionalities and offering guidance on how to effectively utilize this tool.\n\n\nKey Features of SQL Lab\n\nInteractive Query Editor: SQL Lab’s query editor supports syntax highlighting, auto-completion, and error detection, making it easier to write and debug SQL queries.\nQuery History: Keep track of all your queries with SQL Lab’s query history feature. This allows you to revisit, analyze, and repurpose previous queries, enhancing productivity and collaboration.\nData Visualization: After executing a query, SQL Lab enables you to visualize the results directly within the interface. This immediate feedback loop is invaluable for quick iterations and refinements of your analysis.\n\n\n\nGetting Started with SQL Lab\n\nAccess SQL Lab: Navigate to the SQL Lab interface from the main menu of Ortege Studio. Here, you’ll find the query editor, saved queries, and query history.\nConnect to a Database: Before you can start querying, you must connect SQL Lab to a database. Select your database from the dropdown menu of available connections, which are configured in the Database Connections section of Ortege Studio.\nWrite and Execute Your Query: In the query editor, write your SQL query. Execute the query by clicking the “Run Query” button. SQL Lab will process your query and display the results in the lower panel.\n\n\n\nAdvanced Querying in SQL Lab\n\nSaved Queries: SQL Lab allows you to save your queries for future use. This is particularly useful for complex queries that you run regularly or for sharing queries with team members.\nVisualization Options: Explore different visualization options for your query results. You can choose from various chart types and configurations to best represent your data insights.\nExport Results: SQL Lab provides options to export your query results for further analysis or reporting. Supported formats include CSV and Excel, among others.\n\n\n\nBest Practices for Using SQL Lab\n\nOptimize Your Queries: To ensure efficient data retrieval and processing, optimize your SQL queries. This includes selecting only necessary columns, using appropriate WHERE clauses to filter data, and leveraging indexes.\nUse Comments: Document your queries with comments to explain the purpose, logic, and any specific considerations. This practice improves readability and facilitates collaboration.\nUnderstand Your Data: Familiarize yourself with the structure and characteristics of your datasets. Understanding your data will help you craft more effective and accurate queries.\n\n\n\nTroubleshooting Common Issues\n\nPerformance Issues: For queries that take a long time to execute, consider optimizing your query or checking the database performance. Additionally, SQL Lab settings may allow for query time limits and concurrency settings to be adjusted.\nConnectivity Problems: If you encounter issues connecting to a database, verify your connection settings and ensure that the database server is accessible from Ortege Studio.\n\nSQL Lab in Ortege Studio offers a sophisticated environment for direct data querying and analysis, bridging the gap between traditional SQL development and modern data visualization. By leveraging SQL Lab, users can harness the full potential of their data, gaining deeper insights and driving more informed decisions across their organization."
  },
  {
    "objectID": "content/studio/dbconnections.html",
    "href": "content/studio/dbconnections.html",
    "title": "Database Connections",
    "section": "",
    "text": "Database Connections in Ortege Studio\nDatabase connections are a fundamental aspect of Ortege Studio, enabling users to access and interact with data stored across various database systems. This feature provides the foundation for all data analysis, visualization, and dashboard creation within Ortege Studio. This section of the documentation will guide you through the process of setting up, managing, and troubleshooting database connections.\n\n\nSetting Up Database Connections\n\nAccess Connection Settings: Navigate to the database connections section within Ortege Studio’s settings. Here, you’ll find options to add a new connection or modify existing ones.\nProvide Connection Details: To connect to a database, you’ll need to provide specific details, such as the database type (e.g., MySQL, PostgreSQL, Oracle), connection name, and necessary credentials (username, password, server address, port, and database name). Ortege Studio supports a wide range of databases, catering to diverse data storage needs.\nTest and Save: Before finalizing the connection, use the “Test Connection” feature to verify that Ortege Studio can communicate with your database. If successful, save the connection for future use.\n\n\n\nManaging Database Connections\n\nConnection Pooling: Ortege Studio allows you to configure connection pooling, which maintains a pool of database connections that can be reused, improving the efficiency of data operations.\nSecurity and Access Control: Manage who can access each database connection within Ortege Studio. Ensure that only authorized users have access to sensitive data by configuring user permissions and roles.\nUpdating Connections: As your database configurations change, you may need to update connection details. Ortege Studio makes it easy to edit existing connections to reflect new credentials or parameters.\n\n\n\nAdvanced Configuration Options\n\nSSL Encryption: For enhanced security, Ortege Studio supports SSL encryption for database connections. This ensures that data transmitted between Ortege Studio and your database is encrypted and secure.\nSSH Tunneling: If direct connections to your database are not possible due to network restrictions, Ortege Studio supports SSH tunneling as a secure method to connect through an intermediary server.\nQuery Performance: Some databases allow for additional parameters to optimize query performance, such as specifying fetch size or enabling query caching. Explore these options to enhance your data analysis experience.\n\n\n\nBest Practices for Database Connections\n\nRegularly Review Connections: Periodically review and test your database connections to ensure they are functioning correctly and reflect the current database configurations.\nSecure Your Credentials: Use secure methods to store and manage database credentials, avoiding hard-coded credentials within scripts or documents.\nMonitor Usage and Performance: Keep an eye on database performance and usage patterns, especially for connections that support multiple users or high-volume data operations. Adjust pooling settings and query optimization as needed.\n\n\n\nTroubleshooting Common Connection Issues\n\nConnectivity Problems: Verify network settings, firewall rules, and database server availability if you encounter connectivity issues. Ensure that the database server allows connections from Ortege Studio’s IP address.\nAuthentication Failures: Double-check the credentials and authentication methods required by your database. Some databases may require specific configurations or permissions for third-party applications like Ortege Studio.\nTimeouts and Performance: For timeouts or slow query performance, consider optimizing your database indexes, reviewing query efficiency, and adjusting connection or server settings to accommodate higher loads.\n\nDatabase connections are crucial for leveraging the full capabilities of Ortege Studio, providing the gateway to your data. By carefully setting up, managing, and troubleshooting these connections, you ensure a seamless and efficient data analysis workflow within your organization."
  },
  {
    "objectID": "content/studio/overview.html",
    "href": "content/studio/overview.html",
    "title": "Ortege Studio Overview",
    "section": "",
    "text": "Overview of Ortege Studio\nOrtege Studio is a premier analytics platform that empowers users to visualize, explore, and share data insights in a highly interactive and user-friendly environment. Built for versatility and performance, Ortege Studio facilitates the creation of dynamic dashboards and charts, management of extensive datasets, and execution of complex SQL queries. Its seamless integration capabilities with various database connections make it an ideal solution for organizations looking to harness the power of their data. Whether you’re a data analyst, business intelligence professional, or data scientist, Ortege Studio provides the tools you need to transform data into actionable insights.\n\n\nDashboards\nDashboards in Ortege Studio serve as a centralized space where users can create, view, and interact with a collection of related charts and visualizations. These dashboards are customizable, allowing users to tailor the layout and appearance to meet their specific reporting needs. Users can easily share dashboards with stakeholders, providing a snapshot of key metrics and trends at a glance.\n\n\nCharts\nThe Charts section of Ortege Studio is where data comes to life. Users can choose from an extensive library of chart types to best represent their data, from simple line charts to complex scatter plots. This section guides users through the process of creating, customizing, and refining charts, including selecting data sources, applying filters, and adjusting visual aesthetics to highlight critical insights.\n\n\nDatasets\nDatasets are the foundation of any analysis in Ortege Studio. This section explains how to manage and prepare datasets for analysis, including importing data from various sources, creating custom metrics and dimensions, and optimizing data for performance. Users will learn how to leverage datasets to fuel their charts and dashboards, ensuring accurate and up-to-date information is always at their fingertips.\n\n\nSQL Lab\nSQL Lab is Ortege Studio’s interactive environment for crafting and executing SQL queries. It’s designed for users who prefer direct manipulation of their data through SQL. This powerful tool supports query creation, visualization, and sharing, enabling users to explore their data in-depth. The documentation will cover SQL Lab’s features, such as query history, saved queries, and the ability to preview and export results.\n\n\nDatabase Connections\nThe Database Connections section details how to connect Ortege Studio to a variety of data sources, ensuring users can access and analyze data stored in different databases and platforms. It includes step-by-step instructions for establishing connections, managing database credentials securely, and troubleshooting common connection issues. This section is crucial for organizations looking to integrate Ortege Studio into their existing data infrastructure seamlessly."
  },
  {
    "objectID": "content/lakehouse/api.html",
    "href": "content/lakehouse/api.html",
    "title": "API",
    "section": "",
    "text": "Welcome to the Ortege Lakehouse API documentation. Our APIs offer seamless, secure, and efficient access to the Ortege Lakehouse, empowering you to run SQL queries against a vast, curated dataset tailored to meet the evolving needs of your projects. This documentation is designed to guide you through using our APIs to access and manipulate data within the Ortege Lakehouse, ensuring you can harness the full potential of our platform to drive insights and innovation."
  },
  {
    "objectID": "content/lakehouse/api.html#authentication",
    "href": "content/lakehouse/api.html#authentication",
    "title": "API",
    "section": "Authentication",
    "text": "Authentication\nTo begin using the Ortege Lakehouse APIs, you’ll need an API access key. Please reach out to christos@ortege.io in order to obtain an API key.\nOnce you have your access key, you’re ready to make your first API call. Here’s a simple example to get you started:\nCopy code\ncurl -X POST -H \"Authorization: Bearer YOUR_ACCESS_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"sql\": \"SELECT * FROM your_dataset LIMIT 10\"}' \\\n     https://api.ortege.ai/lakehouse/query\nReplace YOUR_ACCESS_KEY with your actual access key and your_dataset with the dataset you wish to query."
  },
  {
    "objectID": "content/lakehouse/api.html#query-data",
    "href": "content/lakehouse/api.html#query-data",
    "title": "API",
    "section": "Query Data",
    "text": "Query Data\n\nEndpoint: /lakehouse/query\nMethod: POST\nDescription: Run SQL queries against datasets in the Ortege Lakehouse.\nRequest Body:\n\nsql: The SQL query string you wish to execute.\n\nResponse: A JSON object containing the query results."
  },
  {
    "objectID": "content/lakehouse/overview.html",
    "href": "content/lakehouse/overview.html",
    "title": "Overview of Ortege Lakehouse",
    "section": "",
    "text": "Ortege Lakehouse represents a cutting-edge data management and analytics platform, meticulously designed to accommodate the vast and growing needs of blockchain data analysis. At the core of our innovation is a seamless integration with OrtegeETL, comprehensive access through Ortege Studio, and exclusive API access for institutional customers. This platform not only underscores our commitment to leveraging open-source software but also ensures that our users have unparalleled access to blockchain datasets."
  },
  {
    "objectID": "content/lakehouse/overview.html#bronze-layer-the-foundation-with-raw-data",
    "href": "content/lakehouse/overview.html#bronze-layer-the-foundation-with-raw-data",
    "title": "Overview of Ortege Lakehouse",
    "section": "Bronze Layer: The Foundation with Raw Data",
    "text": "Bronze Layer: The Foundation with Raw Data\nThe Bronze Layer forms the foundational bedrock of Ortege Lakehouse, containing the raw, unprocessed data directly ingested from multiple blockchains. Utilizing OrtegeETL, this layer captures a wide array of blockchain transactions and metadata, preserving the original integrity and granularity of the data. Ideal for users who need access to the most unadulterated form of blockchain data, the Bronze Layer offers a comprehensive snapshot of blockchain ecosystems in their native state."
  },
  {
    "objectID": "content/lakehouse/overview.html#silver-layer-enhanced-readability-and-transformation",
    "href": "content/lakehouse/overview.html#silver-layer-enhanced-readability-and-transformation",
    "title": "Overview of Ortege Lakehouse",
    "section": "Silver Layer: Enhanced Readability and Transformation",
    "text": "Silver Layer: Enhanced Readability and Transformation\nBuilding upon the raw data stored in the Bronze Layer, the Silver Layer introduces a series of transformations and enhancements to make the data more accessible and human-readable. This layer focuses on cleaning, structuring, and enriching the raw data, applying consistent schemas, and resolving common issues such as data normalization and deduplication. The Silver Layer is designed for users who require a balance between the raw detail of blockchain transactions and the convenience of pre-processed data, enabling more straightforward analysis and exploration."
  },
  {
    "objectID": "content/lakehouse/overview.html#gold-layer-optimized-queries-and-focused-datasets",
    "href": "content/lakehouse/overview.html#gold-layer-optimized-queries-and-focused-datasets",
    "title": "Overview of Ortege Lakehouse",
    "section": "Gold Layer: Optimized Queries and Focused Datasets",
    "text": "Gold Layer: Optimized Queries and Focused Datasets\nThe pinnacle of the Medallion Layer Architecture, the Gold Layer, is where data meets performance and specificity. This layer includes datasets that have been further refined and optimized for specific use cases, themes, or queries. With a focus on performance, the Gold Layer offers pre-built queries, aggregated summaries, and analytical models tailored to particular areas of interest, such as market trends, transaction patterns, or network health indicators. This layer is perfect for users seeking the most efficient access to actionable insights, with data curated and optimized to support rapid analysis and decision-making."
  },
  {
    "objectID": "studio/sqllab.html",
    "href": "studio/sqllab.html",
    "title": "SQL Lab",
    "section": "",
    "text": "SQL Lab\nSQL Lab is a feature-rich SQL editor within Ortege Studio designed for crafting and running complex queries. The interface is user-friendly and supports a variety of functions to enhance your data exploration experience. Let’s take a detailed tour of its interface.\n\nSQL Editor\nScreenshot needed: SQL Editor interface\nThe SQL Editor is the heart of SQL Lab. It’s where you write, edit, and run your SQL queries.\n\nQuery Tab: Located at the top, each tab represents a separate SQL query, allowing you to work on multiple queries simultaneously.\nSQL Editor Pane: This is the large text area where you write your SQL code. It features syntax highlighting and auto-completion to assist with writing queries efficiently.\nRun Button: Click this button to execute the query written in the active tab.\n\n\n\nResults Pane\nScreenshot needed: Results pane showing query results\nBelow the SQL Editor, you’ll find the Results Pane.\n\nData View: Once a query is run, the results are displayed here in a tabular format.\nRow Limit: The interface allows you to specify the number of rows to display, which is useful for large result sets.\nCSV Export: You can export the result of your query directly to a CSV file from this pane.\n\n\n\nData Preview\nScreenshot needed: Data preview of a selected table\n\nData Source Selector: To the left of the SQL Editor, you’ll find the Data Source Selector. Here, you can choose the database and schema you wish to query.\nTable Selector and Preview: Once a table is selected, a preview of the table’s data is displayed, along with column names and data types. This is particularly helpful for understanding the structure of your data before writing a query.\n\n\n\nQuery History\nScreenshot needed: Query history section\n\nAccessing History: The Query History section, accessible from the bottom left of the interface, keeps a log of all executed queries.\nDetails and Actions: Each entry in the history shows the query, the time of execution, and its status. You can also rerun or copy queries from this history.\n\n\n\nAdditional Features\n\nSave Query: You can save queries for future use. This option is located next to the Run button.\nSchema Browser: The Schema Browser, adjacent to the Data Source Selector, lists all tables in the selected schema and provides information about table columns, indexes, and partitions.\n\n\n\nConclusion\nThe SQL Lab interface in Ortege Studio is designed to be intuitive yet powerful, catering to both beginners and experienced SQL users. The seamless integration of query writing, data preview, and result analysis within a single interface makes SQL Lab an efficient tool for data exploration and analysis."
  },
  {
    "objectID": "studio/charts.html",
    "href": "studio/charts.html",
    "title": "Charts",
    "section": "",
    "text": "Charts\nThe Charts feature in Ortege Studio is a powerful tool for creating and managing a wide range of data visualizations. From simple line charts to complex geospatial visualizations, Charts enable users to turn data into insightful and interactive visual representations.\n\nAccessing Charts\nScreenshot needed: Accessing Charts from the Studio menu\n\nNavigation: Charts can be accessed from the main Ortege Studio menu, typically under the “Charts” section.\nCharts Page: This page lists all the created charts, showing details like the chart name, type, and the dataset it’s based on.\n\n\n\nCreating a New Chart\nScreenshot needed: Creating a new chart interface\n\nChart Creation: To create a new chart, click on the “+” or “New” button.\nChoosing a Dataset: First, select the dataset you want to visualize.\nSelecting Chart Type: Ortege Studio offers a variety of chart types. Choose one that best suits the data and the insights you wish to convey.\n\n\n\nChart Customization\nScreenshot needed: Chart customization interface\n\nCustomization Panel: Once you’ve selected the chart type, you’ll be taken to the customization panel. Here, you can define and tweak various aspects of your chart.\nSetting Metrics and Dimensions: Choose the metrics and dimensions from your dataset that you want to visualize.\nChart Options: Adjust settings such as filters, sorting, and chart-specific options to fine-tune your visualization.\n\n\n\nVisualizing the Data\nScreenshot needed: Finished chart visualization\n\nPreviewing the Chart: As you customize your chart, you can preview it in real-time. This feature helps in iteratively building your visualization.\nInteractivity: Many chart types in Ortege Studio are interactive, allowing users to hover over elements to see more details or click to drill down further.\n\n\n\nSaving and Sharing Charts\nScreenshot needed: Saving and sharing options for a chart\n\nSaving the Chart: Once satisfied with the visualization, you can save the chart, giving it a descriptive name and, if necessary, adding it to a dashboard.\nSharing: Ortege Studio allows you to share charts with others, either through direct links or by adding them to shared dashboards.\n\n\n\nAdvanced Features\nScreenshot needed: Advanced features in chart creation, like SQL editing\n\nSQL Editing: For advanced users, Ortege Studio allows editing the underlying SQL of a chart, providing more control over the data and its presentation.\nCustom Visualization Plugins: Users can extend Ortege Studio’s visualization capabilities by adding custom chart plugins.\n\n\n\nConclusion\nThe Charts feature in Ortege Studio is a cornerstone of its data visualization capabilities. It offers a broad spectrum of options to represent data graphically, catering to both novice users and data experts. With its intuitive interface and wide range of customization options, Charts empower users to transform data into actionable insights."
  }
]